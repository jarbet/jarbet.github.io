[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaron Arbet, Ph.D.",
    "section": "",
    "text": "I’m a biostatistician and data scientist working at the UCLA Jonsson Comprehensive Cancer Center in Dr. Paul Boutros’ Cancer Data Science lab.\nI’m passionate about using statistics, data science, and software development to advance cancer research and improve public health."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Jaron Arbet, Ph.D.",
    "section": "",
    "text": "Machine learning and predictive modeling\nHigh-dimensional “Big Data”\nVariable selection\nMulti-omics cancer data integration\nRobust nonparametric statistics"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#why",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#why",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "RCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#real-world",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#real-world",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Real World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#potential-outcomes-causal-framework",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#potential-outcomes-causal-framework",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#example",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#example",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Subject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#propensity-scores-ps",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#propensity-scores-ps",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\tag{1}\\]\n Logistic regression or machine learning to estimate Equation 1"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#why-model-treatment-assignment",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#why-model-treatment-assignment",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Confounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#methods-of-using-ps",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#methods-of-using-ps",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#section",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#section",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "The PS is a balancing score: patients with similar PS should have similar baseline covariates (Austin 2011a)\n\n\n\n\n (McDonald et al. 2013)\n\n\nPS is a composite summary of all baseline covariates\nMatch Trt-Control patients with similar PS values"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#matching-many-choices",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#matching-many-choices",
    "title": "Causal inference with observational data",
    "section": "Matching: many choices",
    "text": "Matching: many choices\nRecommendations from simulations of (Austin 2014a):\n\nOptimal vs greedy nearest neighbor matching?\n\n\n\n\n\n\n(Chen et al. 2022)\n\n\nGreedy: iteratively match to nearest neighbor\nOptimal: minimize the total distance between all pairs"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#caliper-or-no-caliper",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#caliper-or-no-caliper",
    "title": "Causal inference with observational data",
    "section": "Caliper or no caliper?",
    "text": "Caliper or no caliper?\n\n https://help.easymedstat.com/support/solutions/articles/77000538175-caliper-in-propensity-score-matching"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#section-1",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#section-1",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "How wide should caliper be? 0.2*SD of logit PS (Austin 2011b)\n\n\n\nMatch with or without replacement?\n\n\n\nFor greedy matching, what order should you select the treated subjects?\n\nLowest to highest PS, highest to lowest, best match first, or random order"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#types-of-treatment-effects",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#types-of-treatment-effects",
    "title": "Causal inference with observational data",
    "section": "Types of treatment effects",
    "text": "Types of treatment effects\nATE: \\(E\\big[Y_i(T) - Y_i(C)\\big]\\)\n\nAverage effect for ALL patients in the population\n\nATT: \\(E\\big[Y_i(T) - Y_i(C)\\big | T]\\)\n\nAmong patients who were Treated, how would their outcomes have changed if they were a Control?\n\nATC: \\(E\\big[Y_i(T) - Y_i(C)\\big | C]\\)\n\nAmong patients who were Controls, how would their outcomes have changed if they were Treated?\n\n\n\n\nMethod\nATE\nATT/ATC\n\n\n\n\nMatching\n❌\n✅\n\n\nStratification\n✅\n✅\n\n\nInverse probability weighting\n✅\n✅\n\n\nCovariate adjustment\n❌\n❌\n\n\n\n (Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#employment-training-and-income",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#employment-training-and-income",
    "title": "Causal inference with observational data",
    "section": "Employment training and income",
    "text": "Employment training and income\n\nTreatment (N = 185): National Supported Work Demonstration (NSW) employment training program\nControl (N = 429): from the Population Survey of Income Dynamics (PSID)\nGoal: Does training program increase mean income (1978)?\nBaseline covariates: age, education, race, marital status, pre-treatment income (1974/1975)\n\n\n\n(Dehejia and Wahba 1999)\n\ndata(lalonde, package = 'MatchIt');\ndataset &lt;- lalonde;\npsvars &lt;- c('age', 'educ', 'race', 'married', 'nodegree', 're74', 're75');\noutcome &lt;- 're78';\nexposure &lt;- 'treat';\npsdata &lt;- dataset[,c(exposure, psvars)];\nstopifnot(all(c(psvars,outcome, exposure) %in% colnames(dataset)));\n\n# check initial balance\nmatch.null &lt;- matchit(\n    treat~.,\n    data = psdata,\n    method = NULL\n    )\n#summary(match.null);"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#nearest-neighbor-caliper-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#nearest-neighbor-caliper-matching",
    "title": "Causal inference with observational data",
    "section": "Nearest neighbor caliper matching",
    "text": "Nearest neighbor caliper matching\n\nlibrary(MatchIt);\n\nset.seed(1234);\nmatch.nnc.logit &lt;- matchit(\n    treat ~.,\n    data = psdata,\n    method = 'nearest',\n    distance = 'glm',\n    replace = FALSE,\n    caliper = 0.2,\n    std.caliper = TRUE,\n    ratio = 1 # 1:1 matching\n    );\n\n\n\nMatchIt R package (Stuart 2011)\nDefault is logistic reg., but ?distance gives many other options (e.g. LASSO, random forests, boosting, NNets)\n(Austin 2011b) recommends caliper = 0.2*SD logit PS"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#ps-distribution-beforeafter-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#ps-distribution-beforeafter-matching",
    "title": "Causal inference with observational data",
    "section": "PS distribution before/after matching",
    "text": "PS distribution before/after matching\n\n\nbal.plot(\n    x = match.nnc.logit,\n    var.name = 'distance',\n    which = 'both',\n    sample.names = c('Unmatched', 'Matched')\n    );\n\n\n\n\n\n\nNotice poor overlap before matching ➔ won’t match all Trt patients, unless \\(N_{control}\\) is large\n\n\ntab &lt;- matchit.pct.matched(match.nnc.logit)$tab.counts.pct;\nknitr::kable(tab);\n\n\n\n\n\nControl\nTreated\n\n\n\n\nTotal\n429\n185\n\n\nMatched\n113 (26.3%)\n113 (61.1%)\n\n\nUnmatched\n316 (73.7%)\n72 (38.9%)\n\n\n\n\n\n\n⬆ unmatched Trt patients = ⬆ biased \\(\\widehat{\\text{ATT}}\\), instead estimates “Average treatment effect in the Overlap population (ATO)” (Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#covariate-balance",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#covariate-balance",
    "title": "Causal inference with observational data",
    "section": "Covariate balance",
    "text": "Covariate balance\n\n\nplot(\n    match.nnc.logit,\n    type = 'density',\n    interactive = FALSE\n    );"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#standardized-differences",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#standardized-differences",
    "title": "Causal inference with observational data",
    "section": "Standardized differences",
    "text": "Standardized differences\n\n\n\nplot(summary(match.nnc.logit, un = TRUE), threshold = c(0.1, 0.25));\n\n\n\n\n\n\n\nCutoffs:\n\n\\(\\le\\) 0.1 (Austin 2011a)\n\\(\\le\\) 0.25 (Harder 2010)\nNo p-values (Austin 2011a)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#estimated-treatment-effect",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#estimated-treatment-effect",
    "title": "Causal inference with observational data",
    "section": "Estimated treatment effect",
    "text": "Estimated treatment effect\n\nmdata &lt;- match.data(match.nnc.logit);\ncolnames(mdata)[colnames(mdata) == 'subclass'] &lt;- 'match.id';\nmdata$patient.id &lt;- rownames(mdata);\nodata &lt;- dataset[, outcome, drop = FALSE];\nodata$patient.id &lt;- rownames(odata);\n\nmdata &lt;- merge(\n    x = mdata,\n    y = odata,\n    by = 'patient.id',\n    all.x = TRUE\n    );\n\n\ncreate.densityplot(\n    x = data.frame(mdata$re78[mdata$treat==1], mdata$re78[mdata$treat==0]),\n    xlab.label = 'Income in 1978 (USD)',\n    xlimits = c(0, 30000),\n    xat = seq(0, 30000, by = 5000),\n    col = default.colours(2),\n    # Legend\n    legend = list(\n        inside = list(\n            fun = draw.key,\n            args = list(\n                key = list(\n                    points = list(\n                        col = default.colours(2),\n                        pch = 21,\n                        cex = 1.5,\n                        fill = default.colours(2)\n                        ),\n                    text = list(\n                        lab = c('Treatment', 'Control')\n                        ),\n                    padding.text = c(0,5,0),\n                    cex = 1\n                    )\n                ),\n            x = 0.65,\n            y = 0.97,\n            draw = FALSE\n            )\n        ),\n    );\n\n\n\n\n\n\nmean.diff &lt;- t.test(re78 ~ factor(treat, levels = c(1, 0)), data = mdata);\nmean.diff &lt;- broom::tidy(mean.diff);\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate'] &lt;- 'diff'\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate1'] &lt;- 'trt.mean'\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate2'] &lt;- 'ctrl.mean'\nmean.diff &lt;- mean.diff[c('trt.mean', 'ctrl.mean', 'diff', 'conf.low', 'conf.high', 'p.value')];\n\nknitr::kable(\n    mean.diff,\n    digits = c(rep(0, 5), 3),\n    table.attr = \"style='width:50%;'\"\n    );\n\n\n\n\n\n\n\n\n\n\n\n\ntrt.mean\nctrl.mean\ndiff\nconf.low\nconf.high\np.value\n\n\n\n\n6510\n4938\n1572\n-365\n3508\n0.111\n\n\n\n\n\n\n\n Interestingly, PS estimate of trt effect ($1572) is close to RCT estimate ($1641) (LaLonde 1986)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#optimal-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#optimal-matching",
    "title": "Causal inference with observational data",
    "section": "Optimal matching",
    "text": "Optimal matching\n\n\nset.seed(seed);\nmatch.opt &lt;- matchit(\n    treat~.,\n    data = psdata,\n    method = 'optimal',\n    distance = 'glm',\n    # estimand: set to ATT if N_treat &gt;&gt; N_control, \n    #   ATC if N_control &gt;&gt; N_treat\n    estimand = 'ATT',\n    replace = FALSE,\n    ratio = 1 # 1:1 matching\n    );\nplot(summary(match.opt, un = TRUE), threshold = c(0.1, 0.25));\n\n\n\n\n\n\nMatches 100% of Trt patients (unlike greedy caliper match)\nAttempts to optimize the total distance between all pairs\nHowever, resulted in much worse covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#summary",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#summary",
    "title": "Causal inference with observational data",
    "section": "Summary",
    "text": "Summary\n\nGoal: estimate causal effect of exposure \\(X\\) on outcome \\(Y\\), using observational data\nPS matching balances measured confounders between exposure groups (similar to RCT)\n\n Intuitive: show covariates are balanced after matching, like RCT \n Of course, unmeasured confounders may remain \n\nIf estimating ATT (ATC), then need to match all Trt (Control) patients. If high % unmatched, consider other methods."
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#alternatives",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html#alternatives",
    "title": "Causal inference with observational data",
    "section": "Alternatives",
    "text": "Alternatives\n\nStratification on PS uses all patients, but does not work well with survival data (Austin 2014b, 2016)\nInverse probability weighting and regression adjustment are very flexible, but generally not accepted by FDA (unlike matching) (Lu 2019)\nCausal Random Forests look promising: https://grf-labs.github.io/grf/"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-observational-data-matching.html",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "library(MatchIt);\nlibrary(cobalt);\n\n cobalt (Version 4.5.0, Build Date: 2023-03-21)\n\n\n\nAttaching package: 'cobalt'\n\n\nThe following object is masked from 'package:MatchIt':\n\n    lalonde\n\nlibrary(BoutrosLab.plotting.general);\n\nLoading required package: lattice\n\n\nLoading required package: latticeExtra\n\n\nLoading required package: cluster\n\n\nLoading required package: hexbin\n\n\nLoading required package: grid\n\n\n\nAttaching package: 'BoutrosLab.plotting.general'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\nseed &lt;- 1234;\n\nsource('utilities.R')\n\n\n\nRCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)\n\n\n\nReal World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA\n\n\n\n\n\n2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)\n\n\n\n\n\n\n“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\tag{1}\\]\n Logistic regression or machine learning to estimate Equation 1\n\n\n\n\n\nConfounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000) \n\n\n\n\n\n\n\n(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#why",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#why",
    "title": "Causal inference with observational data",
    "section": "Why?",
    "text": "Why?\nRCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#real-world",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#real-world",
    "title": "Causal inference with observational data",
    "section": "Real World",
    "text": "Real World\nReal World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#potential-outcomes-causal-framework",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#potential-outcomes-causal-framework",
    "title": "Causal inference with observational data",
    "section": "Potential outcomes causal framework",
    "text": "Potential outcomes causal framework\n\n2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#example",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#example",
    "title": "Causal inference with observational data",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#propensity-scores-ps",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#propensity-scores-ps",
    "title": "Causal inference with observational data",
    "section": "Propensity scores (PS)",
    "text": "Propensity scores (PS)\n\n“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\qquad(1)\\]\n Logistic regression or machine learning to estimate Equation 1"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#why-model-treatment-assignment",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#why-model-treatment-assignment",
    "title": "Causal inference with observational data",
    "section": "Why model treatment assignment?",
    "text": "Why model treatment assignment?\n\n\nConfounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#methods-of-using-ps",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#methods-of-using-ps",
    "title": "Causal inference with observational data",
    "section": "4 methods of using PS",
    "text": "4 methods of using PS\n\n\n\n(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#section",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#section",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "The PS is a balancing score: patients with similar PS should have similar baseline covariates (Austin 2011a)\n\n\n\n\n (McDonald et al. 2013)\n\n\nPS is a composite summary of all baseline covariates\nMatch Trt-Control patients with similar PS values"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#matching-many-choices",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#matching-many-choices",
    "title": "Causal inference with observational data",
    "section": "Matching: many choices",
    "text": "Matching: many choices\nRecommendations from simulations of (Austin 2014a):\n\nOptimal vs greedy nearest neighbor matching?\n\n\n\n\n\n\n\nGreedy: iteratively match to nearest neighbor\nOptimal: minimize the total distance between all pairs\n\n\n\n\n(Chen et al. 2022)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#caliper-or-no-caliper",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#caliper-or-no-caliper",
    "title": "Causal inference with observational data",
    "section": "Caliper or no caliper?",
    "text": "Caliper or no caliper?\n\n https://help.easymedstat.com/support/solutions/articles/77000538175-caliper-in-propensity-score-matching"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#section-1",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#section-1",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "How wide should caliper be? 0.2*SD of logit PS (Austin 2011b)\n\n\n\nMatch with or without replacement?\n\n\n\nFor greedy matching, what order should you select the treated subjects?\n\nLowest to highest PS, highest to lowest, best match first, or random order"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#types-of-treatment-effects",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#types-of-treatment-effects",
    "title": "Causal inference with observational data",
    "section": "Types of treatment effects",
    "text": "Types of treatment effects\nATE: \\(E\\big[Y_i(T) - Y_i(C)\\big]\\)\n\nAverage effect for ALL patients in the population\n\nATT: \\(E\\big[Y_i(T) - Y_i(C)\\big | T]\\)\n\nAmong patients who were Treated, how would their outcomes have changed if they were a Control?\n\nATC: \\(E\\big[Y_i(T) - Y_i(C)\\big | C]\\)\n\nAmong patients who were Controls, how would their outcomes have changed if they were Treated?\n\n\n\n\nMethod\nATE\nATT/ATC\n\n\n\n\nMatching\n❌\n✅\n\n\nStratification\n✅\n✅\n\n\nInverse probability weighting\n✅\n✅\n\n\nCovariate adjustment\n❌\n❌\n\n\n\n (Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#employment-training-and-income",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#employment-training-and-income",
    "title": "Causal inference with observational data",
    "section": "Employment training and income",
    "text": "Employment training and income\n\nTreatment (N = 185): National Supported Work Demonstration (NSW) employment training program\nControl (N = 429): from the Population Survey of Income Dynamics (PSID)\nGoal: Does training program increase mean income (1978)?\nBaseline covariates: age, education, race, marital status, pre-treatment income (1974/1975)\n\n\n\ndata(lalonde, package = 'MatchIt');\ndataset &lt;- lalonde;\npsvars &lt;- c('age', 'educ', 'race', 'married', 'nodegree', 're74', 're75');\noutcome &lt;- 're78';\nexposure &lt;- 'treat';\npsdata &lt;- dataset[,c(exposure, psvars)];\nstopifnot(all(c(psvars,outcome, exposure) %in% colnames(dataset)));\n\n# check initial balance\nmatch.null &lt;- matchit(\n    treat~.,\n    data = psdata,\n    method = NULL\n    )\n#summary(match.null);\n\n\n(Dehejia and Wahba 1999)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#nearest-neighbor-caliper-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#nearest-neighbor-caliper-matching",
    "title": "Causal inference with observational data",
    "section": "Nearest neighbor caliper matching",
    "text": "Nearest neighbor caliper matching\n\nlibrary(MatchIt);\n\nset.seed(1234);\nmatch.nnc.logit &lt;- matchit(\n    treat ~.,\n    data = psdata,\n    method = 'nearest',\n    distance = 'glm',\n    replace = FALSE,\n    caliper = 0.2,\n    std.caliper = TRUE,\n    ratio = 1 # 1:1 matching\n    );\n\n\n\nMatchIt R package (Stuart 2011)\nDefault is logistic reg., but ?distance gives many other options (e.g. LASSO, random forests, boosting, NNets)\n(Austin 2011b) recommends caliper = 0.2*SD logit PS"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#ps-distribution-beforeafter-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#ps-distribution-beforeafter-matching",
    "title": "Causal inference with observational data",
    "section": "PS distribution before/after matching",
    "text": "PS distribution before/after matching\n\n\nbal.plot(\n    x = match.nnc.logit,\n    var.name = 'distance',\n    which = 'both',\n    sample.names = c('Unmatched', 'Matched')\n    );\n\n\n\n\n\n\nNotice poor overlap before matching ➔ won’t match all Trt patients, unless \\(N_{control}\\) is large\n\n\ntab &lt;- matchit.pct.matched(match.nnc.logit)$tab.counts.pct;\nknitr::kable(tab);\n\n\n\n\n\nControl\nTreated\n\n\n\n\nTotal\n429\n185\n\n\nMatched\n113 (26.3%)\n113 (61.1%)\n\n\nUnmatched\n316 (73.7%)\n72 (38.9%)\n\n\n\n\n\n\n⬆ unmatched Trt patients = ⬆ biased \\(\\widehat{\\text{ATT}}\\), instead estimates “Average treatment effect in the Overlap population (ATO)” (Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#covariate-balance",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#covariate-balance",
    "title": "Causal inference with observational data",
    "section": "Covariate balance",
    "text": "Covariate balance\n\n\nplot(\n    match.nnc.logit,\n    type = 'density',\n    interactive = FALSE\n    );"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#standardized-differences",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#standardized-differences",
    "title": "Causal inference with observational data",
    "section": "Standardized differences",
    "text": "Standardized differences\n\n\n\nplot(summary(match.nnc.logit, un = TRUE), threshold = c(0.1, 0.25));\n\n\n\n\n\n\n\nCutoffs:\n\n\\(\\le\\) 0.1 (Austin 2011a)\n\\(\\le\\) 0.25 (Harder 2010)\nNo p-values (Austin 2011a)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#estimated-treatment-effect",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#estimated-treatment-effect",
    "title": "Causal inference with observational data",
    "section": "Estimated treatment effect",
    "text": "Estimated treatment effect\n\nmdata &lt;- match.data(match.nnc.logit);\ncolnames(mdata)[colnames(mdata) == 'subclass'] &lt;- 'match.id';\nmdata$patient.id &lt;- rownames(mdata);\nodata &lt;- dataset[, outcome, drop = FALSE];\nodata$patient.id &lt;- rownames(odata);\n\nmdata &lt;- merge(\n    x = mdata,\n    y = odata,\n    by = 'patient.id',\n    all.x = TRUE\n    );\n\n\ncreate.densityplot(\n    x = data.frame(mdata$re78[mdata$treat==1], mdata$re78[mdata$treat==0]),\n    xlab.label = 'Income in 1978 (USD)',\n    xlimits = c(0, 30000),\n    xat = seq(0, 30000, by = 5000),\n    col = default.colours(2),\n    # Legend\n    legend = list(\n        inside = list(\n            fun = draw.key,\n            args = list(\n                key = list(\n                    points = list(\n                        col = default.colours(2),\n                        pch = 21,\n                        cex = 1.5,\n                        fill = default.colours(2)\n                        ),\n                    text = list(\n                        lab = c('Treatment', 'Control')\n                        ),\n                    padding.text = c(0,5,0),\n                    cex = 1\n                    )\n                ),\n            x = 0.65,\n            y = 0.97,\n            draw = FALSE\n            )\n        ),\n    );\n\n\n\n\nmean.diff &lt;- t.test(re78 ~ factor(treat, levels = c(1, 0)), data = mdata);\nmean.diff &lt;- broom::tidy(mean.diff);\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate'] &lt;- 'diff'\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate1'] &lt;- 'trt.mean'\ncolnames(mean.diff)[colnames(mean.diff) == 'estimate2'] &lt;- 'ctrl.mean'\nmean.diff &lt;- mean.diff[c('trt.mean', 'ctrl.mean', 'diff', 'conf.low', 'conf.high', 'p.value')];\n\nknitr::kable(\n    mean.diff,\n    digits = c(rep(0, 5), 3),\n    table.attr = \"style='width:50%;'\"\n    );\n\n\n\n\n\n\n\n\n\n\n\n\ntrt.mean\nctrl.mean\ndiff\nconf.low\nconf.high\np.value\n\n\n\n\n6510\n4938\n1572\n-365\n3508\n0.111\n\n\n\n\n\n\n\n Interestingly, PS estimate of trt effect ($1572) is close to RCT estimate ($1641) (LaLonde 1986)"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#optimal-matching",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#optimal-matching",
    "title": "Causal inference with observational data",
    "section": "Optimal matching",
    "text": "Optimal matching\n\n\nset.seed(seed);\nmatch.opt &lt;- matchit(\n    treat~.,\n    data = psdata,\n    method = 'optimal',\n    distance = 'glm',\n    # estimand: set to ATT if N_treat &gt;&gt; N_control, \n    #   ATC if N_control &gt;&gt; N_treat\n    estimand = 'ATT',\n    replace = FALSE,\n    ratio = 1 # 1:1 matching\n    );\nplot(summary(match.opt, un = TRUE), threshold = c(0.1, 0.25));\n\n\n\n\n\n\nMatches 100% of Trt patients (unlike greedy caliper match)\nAttempts to optimize the total distance between all pairs\nHowever, resulted in much worse covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#summary",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#summary",
    "title": "Causal inference with observational data",
    "section": "Summary",
    "text": "Summary\n\nGoal: estimate causal effect of exposure \\(X\\) on outcome \\(Y\\), using observational data\nPS matching balances measured confounders between exposure groups (similar to RCT)\n\n Intuitive: show covariates are balanced after matching, like RCT \n Of course, unmeasured confounders may remain \n\nIf estimating ATT (ATC), then need to match all Trt (Control) patients. If high % unmatched, consider other methods."
  },
  {
    "objectID": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#alternatives",
    "href": "presentations/causal-inference-observational-data-matching/causal-inference-matching-intro.html#alternatives",
    "title": "Causal inference with observational data",
    "section": "Alternatives",
    "text": "Alternatives\n\nStratification on PS uses all patients, but does not work well with survival data (Austin 2014b, 2016)\nInverse probability weighting and regression adjustment are very flexible, but generally not accepted by FDA (unlike matching) (Lu 2019)\nCausal Random Forests look promising: https://grf-labs.github.io/grf/"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Presentations\n\n2023-12-15: Nonparametric feature selection for big data with MARS\n2023-09-24: Linear regression diagnostics\n2023-04-28: Causal inference with observational data using propensity score matching\n2022-08-25: Introduction to Bayesian statistics\n2022-04-22: Triplet matching: propensity score matching with 3 groups\n2021-11-21: Multiomics cancer data analysis\n2020-01-21: Interpretable machine learning"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "RCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)\n\n\n\nReal World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA\n\n\n\n\n\n2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)\n\n\n\n\n\n\n“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\tag{1}\\]\n Logistic regression or machine learning to estimate Equation 1\n\n\n\n\n\nConfounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000) \n\n\n\n\n\n\n\n(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#why",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#why",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "RCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#real-world",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#real-world",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Real World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#potential-outcomes-causal-framework",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#potential-outcomes-causal-framework",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#example",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#example",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Subject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#propensity-scores-ps",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#propensity-scores-ps",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\tag{1}\\]\n Logistic regression or machine learning to estimate Equation 1"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#why-model-treatment-assignment",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#why-model-treatment-assignment",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "Confounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#methods-of-using-ps",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#methods-of-using-ps",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#section",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#section",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "The PS is a balancing score: patients with similar PS should have similar baseline covariates (Austin 2011a)\n\n\n\n\n (McDonald et al. 2013)\n\n\nPS is a composite summary of all baseline covariates\nMatch Trt-Control patients with similar PS values"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#matching-many-choices",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#matching-many-choices",
    "title": "Causal inference with observational data",
    "section": "Matching: many choices",
    "text": "Matching: many choices\nRecommendations from simulations of (Austin 2014a):\n\nOptimal vs greedy nearest neighbor matching?\n\n\n\n\n\n\n(Chen et al. 2022)\n\n\nGreedy: iteratively match to nearest neighbor\nOptimal: minimize the total distance between all pairs"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#caliper-or-no-caliper",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#caliper-or-no-caliper",
    "title": "Causal inference with observational data",
    "section": "Caliper or no caliper?",
    "text": "Caliper or no caliper?\n\n https://help.easymedstat.com/support/solutions/articles/77000538175-caliper-in-propensity-score-matching"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#section-1",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#section-1",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "How wide should caliper be? 0.2*SD of logit PS (Austin 2011b)\n\n\n\nMatch with or without replacement?\n\n\n\nFor greedy matching, what order should you select the treated subjects?\n\nLowest to highest PS, highest to lowest, best match first, or random order"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#types-of-treatment-effects",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#types-of-treatment-effects",
    "title": "Causal inference with observational data",
    "section": "Types of treatment effects",
    "text": "Types of treatment effects\nATE: \\(E\\big[Y_i(T) - Y_i(C)\\big]\\)\n\nAverage effect for ALL patients in the population\n\nATT: \\(E\\big[Y_i(T) - Y_i(C)\\big | T]\\)\n\nAmong patients who were Treated, how would their outcomes have changed if they were a Control?\n\nATC: \\(E\\big[Y_i(T) - Y_i(C)\\big | C]\\)\n\nAmong patients who were Controls, how would their outcomes have changed if they were Treated?\n\n\n\n\nMethod\nATE\nATT/ATC\n\n\n\n\nMatching\n❌\n✅\n\n\nStratification\n✅\n✅\n\n\nInverse probability weighting\n✅\n✅\n\n\nCovariate adjustment\n❌\n❌\n\n\n\n (Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#employment-training-and-income",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#employment-training-and-income",
    "title": "Causal inference with observational data",
    "section": "Employment training and income",
    "text": "Employment training and income\n\nTreatment (N = 185): National Supported Work Demonstration (NSW) employment training program\nControl (N = 429): from the Population Survey of Income Dynamics (PSID)\nGoal: Does training program increase mean income (1978)?\nBaseline covariates: age, education, race, marital status, pre-treatment income (1974/1975)\n\n\n\n(Dehejia and Wahba 1999)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#nearest-neighbor-caliper-matching",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#nearest-neighbor-caliper-matching",
    "title": "Causal inference with observational data",
    "section": "Nearest neighbor caliper matching",
    "text": "Nearest neighbor caliper matching\n\nlibrary(MatchIt);\n\nset.seed(1234);\nmatch.nnc.logit &lt;- matchit(\n    treat ~.,\n    data = psdata,\n    method = 'nearest',\n    distance = 'glm',\n    replace = FALSE,\n    caliper = 0.2,\n    std.caliper = TRUE,\n    ratio = 1 # 1:1 matching\n    );\n\n\n\nMatchIt R package (Stuart 2011)\nDefault is logistic reg., but ?distance gives many other options (e.g. LASSO, random forests, boosting, NNets)\n(Austin 2011b) recommends caliper = 0.2*SD logit PS"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#ps-distribution-beforeafter-matching",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#ps-distribution-beforeafter-matching",
    "title": "Causal inference with observational data",
    "section": "PS distribution before/after matching",
    "text": "PS distribution before/after matching\n\n\n\n\n\n\n\n\nNotice poor overlap before matching ➔ won’t match all Trt patients, unless \\(N_{control}\\) is large\n\n\n\n\n\n\n\nControl\nTreated\n\n\n\n\nTotal\n429\n185\n\n\nMatched\n113 (26.3%)\n113 (61.1%)\n\n\nUnmatched\n316 (73.7%)\n72 (38.9%)\n\n\n\n\n\n\n⬆ unmatched Trt patients = ⬆ biased \\(\\widehat{\\text{ATT}}\\), instead estimates “Average treatment effect in the Overlap population (ATO)” (Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#covariate-balance",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#covariate-balance",
    "title": "Causal inference with observational data",
    "section": "Covariate balance",
    "text": "Covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#standardized-differences",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#standardized-differences",
    "title": "Causal inference with observational data",
    "section": "Standardized differences",
    "text": "Standardized differences\n\n\n\n\n\n\n\n\n\n\nCutoffs:\n\n\\(\\le\\) 0.1 (Austin 2011a)\n\\(\\le\\) 0.25 (Harder 2010)\nNo p-values (Austin 2011a)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#estimated-treatment-effect",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#estimated-treatment-effect",
    "title": "Causal inference with observational data",
    "section": "Estimated treatment effect",
    "text": "Estimated treatment effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrt.mean\nctrl.mean\ndiff\nconf.low\nconf.high\np.value\n\n\n\n\n6510\n4938\n1572\n-365\n3508\n0.111\n\n\n\n\n\n\n\n Interestingly, PS estimate of trt effect ($1572) is close to RCT estimate ($1641) (LaLonde 1986)"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#optimal-matching",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#optimal-matching",
    "title": "Causal inference with observational data",
    "section": "Optimal matching",
    "text": "Optimal matching\n\n\n\n\n\n\n\n\nMatches 100% of Trt patients (unlike greedy caliper match)\nAttempts to optimize the total distance between all pairs\nHowever, resulted in much worse covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#summary",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#summary",
    "title": "Causal inference with observational data",
    "section": "Summary",
    "text": "Summary\n\nGoal: estimate causal effect of exposure \\(X\\) on outcome \\(Y\\), using observational data\nPS matching balances measured confounders between exposure groups (similar to RCT)\n\n Intuitive: show covariates are balanced after matching, like RCT \n Of course, unmeasured confounders may remain \n\nIf estimating ATT (ATC), then need to match all Trt (Control) patients. If high % unmatched, consider other methods."
  },
  {
    "objectID": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#alternatives",
    "href": "presentations/causal-inference-matching/causal-inference-observational-data-matching.html#alternatives",
    "title": "Causal inference with observational data",
    "section": "Alternatives",
    "text": "Alternatives\n\nStratification on PS uses all patients, but does not work well with survival data (Austin 2014b, 2016)\nInverse probability weighting and regression adjustment are very flexible, but generally not accepted by FDA (unlike matching) (Lu 2019)\nCausal Random Forests look promising: https://grf-labs.github.io/grf/"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#why",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#why",
    "title": "Causal inference with observational data",
    "section": "Why?",
    "text": "Why?\nRCTs are expensive:\n\nMedian cost of Phase 3 trials: $19 million (IQR: $12.2 - $33.1M)\nAvg. cost of bringing new drug to market ~ 1 - 3 billion\nMay not generalize to larger “real world” population\n\nMay not be practical or ethically feasible\n\n\n(Moore et al. 2018; Wouters, McKee, and Luyten 2020)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#real-world",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#real-world",
    "title": "Causal inference with observational data",
    "section": "Real World",
    "text": "Real World\nReal World Data RWD:\n\nObservational, collected in “real world” setting\nEHR database, hospital visit notes, wearable devices\nMedical claims/billing database, disease registries\n\nReal World Evidence RWE:\n\nClinical evidence about the benefits/harms of medical products derived from analyzing RWD (Franklin et al. 2021)\nPoor quality RWD: garbage-in-garbage-out\n90 examples of RWE used by FDA"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#potential-outcomes-causal-framework",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#potential-outcomes-causal-framework",
    "title": "Causal inference with observational data",
    "section": "Potential outcomes causal framework",
    "text": "Potential outcomes causal framework\n\n2 exposure groups: e.g. Treatment vs Control\nHow does the exposure affect a given outcome \\(Y\\)?\nThe \\(i\\)th subject has 2 potential outcomes: \\[Y_i(T) \\text{ and } Y_i(C)\\]\nFor each subject, the treatment effect is defined as: \\[Y_i(T) - Y_i(C)\\]\n\nOnly 1 of these is observed in reality\n\n\n(Little and Rubin 2000; Rubin 2005)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#example",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#example",
    "title": "Causal inference with observational data",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(Y_i(T)\\)\n\\(Y_i(C)\\)\nTrt. Effect: \\(Y_i(T) - Y_i(C)\\)\n\n\n\n\n1\n14\n?\n?\n\n\n2\n9\n?\n?\n\n\n3\n8\n?\n?\n\n\n4\n?\n5\n?\n\n\n5\n?\n10\n?\n\n\n6\n?\n7\n?\n\n\nMean\n10.33\n7.33\n3\n\n\n\n\nIn RCT, \\(\\tau = \\bar{Y}_i(T) - \\bar{Y}_i(C)\\) estimates a causal effect\nIn general, \\(\\tau\\) is not causal for observational studies (OS)\n\nMany methods try to change this (Austin 2011a; Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#propensity-scores-ps",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#propensity-scores-ps",
    "title": "Causal inference with observational data",
    "section": "Propensity scores (PS)",
    "text": "Propensity scores (PS)\n\n“Probability of treatment assignment based on observed baseline covariates” (Rosenbaum and Rubin 1983)\n\n\nGiven treatment variable \\(X_i \\in \\{0,1\\}\\) and baseline covariates \\(\\boldsymbol{Z}_i\\), then estimate PS:\n\n\\[\nPS_i = Pr(X_i = 1) = f(\\boldsymbol{Z}_i) + \\epsilon_i\n\\qquad(1)\\]\n Logistic regression or machine learning to estimate Equation 1"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#why-model-treatment-assignment",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#why-model-treatment-assignment",
    "title": "Causal inference with observational data",
    "section": "Why model treatment assignment?",
    "text": "Why model treatment assignment?\n\n\nConfounder \\(\\boldsymbol{Z}\\) causes trt \\(\\boldsymbol{X}\\) and outcome \\(\\boldsymbol{Y}\\)\n\nThis makes \\(\\boldsymbol{X}\\) correlated with \\(\\boldsymbol{Y}\\), but correlation \\(\\neq\\) causation\n\nPS models the relation between \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\), thus removes confounding\n\n  https://sixsigmadsi.com/glossary/confounding/\n\n\n\n Without a model for how treatments are assigned to units, formal causal inference is impossible (Little and Rubin 2000)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#methods-of-using-ps",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#methods-of-using-ps",
    "title": "Causal inference with observational data",
    "section": "4 methods of using PS",
    "text": "4 methods of using PS\n\n\n\n(Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#section",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#section",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "The PS is a balancing score: patients with similar PS should have similar baseline covariates (Austin 2011a)\n\n\n\n\n (McDonald et al. 2013)\n\n\nPS is a composite summary of all baseline covariates\nMatch Trt-Control patients with similar PS values"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#matching-many-choices",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#matching-many-choices",
    "title": "Causal inference with observational data",
    "section": "Matching: many choices",
    "text": "Matching: many choices\nRecommendations from simulations of (Austin 2014a):\n\nOptimal vs greedy nearest neighbor matching?\n\n\n\n\n\n\n\nGreedy: iteratively match to nearest neighbor\nOptimal: minimize the total distance between all pairs\n\n\n\n\n(Chen et al. 2022)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#caliper-or-no-caliper",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#caliper-or-no-caliper",
    "title": "Causal inference with observational data",
    "section": "Caliper or no caliper?",
    "text": "Caliper or no caliper?\n\n https://help.easymedstat.com/support/solutions/articles/77000538175-caliper-in-propensity-score-matching"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#section-1",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#section-1",
    "title": "Causal inference with observational data",
    "section": "",
    "text": "How wide should caliper be? 0.2*SD of logit PS (Austin 2011b)\n\n\n\nMatch with or without replacement?\n\n\n\nFor greedy matching, what order should you select the treated subjects?\n\nLowest to highest PS, highest to lowest, best match first, or random order"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#types-of-treatment-effects",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#types-of-treatment-effects",
    "title": "Causal inference with observational data",
    "section": "Types of treatment effects",
    "text": "Types of treatment effects\nATE: \\(E\\big[Y_i(T) - Y_i(C)\\big]\\)\n\nAverage effect for ALL patients in the population\n\nATT: \\(E\\big[Y_i(T) - Y_i(C)\\big | T]\\)\n\nAmong patients who were Treated, how would their outcomes have changed if they were a Control?\n\nATC: \\(E\\big[Y_i(T) - Y_i(C)\\big | C]\\)\n\nAmong patients who were Controls, how would their outcomes have changed if they were Treated?\n\n\n\n\nMethod\nATE\nATT/ATC\n\n\n\n\nMatching\n❌\n✅\n\n\nStratification\n✅\n✅\n\n\nInverse probability weighting\n✅\n✅\n\n\nCovariate adjustment\n❌\n❌\n\n\n\n (Deb et al. 2016)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#employment-training-and-income",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#employment-training-and-income",
    "title": "Causal inference with observational data",
    "section": "Employment training and income",
    "text": "Employment training and income\n\nTreatment (N = 185): National Supported Work Demonstration (NSW) employment training program\nControl (N = 429): from the Population Survey of Income Dynamics (PSID)\nGoal: Does training program increase mean income (1978)?\nBaseline covariates: age, education, race, marital status, pre-treatment income (1974/1975)\n\n\n\n(Dehejia and Wahba 1999)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#nearest-neighbor-caliper-matching",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#nearest-neighbor-caliper-matching",
    "title": "Causal inference with observational data",
    "section": "Nearest neighbor caliper matching",
    "text": "Nearest neighbor caliper matching\n\nlibrary(MatchIt);\n\nset.seed(1234);\nmatch.nnc.logit &lt;- matchit(\n    treat ~.,\n    data = psdata,\n    method = 'nearest',\n    distance = 'glm',\n    replace = FALSE,\n    caliper = 0.2,\n    std.caliper = TRUE,\n    ratio = 1 # 1:1 matching\n    );\n\n\n\nMatchIt R package (Stuart 2011)\nDefault is logistic reg., but ?distance gives many other options (e.g. LASSO, random forests, boosting, NNets)\n(Austin 2011b) recommends caliper = 0.2*SD logit PS"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#ps-distribution-beforeafter-matching",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#ps-distribution-beforeafter-matching",
    "title": "Causal inference with observational data",
    "section": "PS distribution before/after matching",
    "text": "PS distribution before/after matching\n\n\n\n\n\n\n\n\nNotice poor overlap before matching ➔ won’t match all Trt patients, unless \\(N_{control}\\) is large\n\n\n\n\n\n\n\nControl\nTreated\n\n\n\n\nTotal\n429\n185\n\n\nMatched\n113 (26.3%)\n113 (61.1%)\n\n\nUnmatched\n316 (73.7%)\n72 (38.9%)\n\n\n\n\n\n\n⬆ unmatched Trt patients = ⬆ biased \\(\\widehat{\\text{ATT}}\\), instead estimates “Average treatment effect in the Overlap population (ATO)” (Varga et al. 2023)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#covariate-balance",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#covariate-balance",
    "title": "Causal inference with observational data",
    "section": "Covariate balance",
    "text": "Covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#standardized-differences",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#standardized-differences",
    "title": "Causal inference with observational data",
    "section": "Standardized differences",
    "text": "Standardized differences\n\n\n\n\n\n\n\n\n\n\nCutoffs:\n\n\\(\\le\\) 0.1 (Austin 2011a)\n\\(\\le\\) 0.25 (Harder 2010)\nNo p-values (Austin 2011a)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#estimated-treatment-effect",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#estimated-treatment-effect",
    "title": "Causal inference with observational data",
    "section": "Estimated treatment effect",
    "text": "Estimated treatment effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrt.mean\nctrl.mean\ndiff\nconf.low\nconf.high\np.value\n\n\n\n\n6510\n4938\n1572\n-365\n3508\n0.111\n\n\n\n\n\n\n\n Interestingly, PS estimate of trt effect ($1572) is close to RCT estimate ($1641) (LaLonde 1986)"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#optimal-matching",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#optimal-matching",
    "title": "Causal inference with observational data",
    "section": "Optimal matching",
    "text": "Optimal matching\n\n\n\n\n\n\n\n\nMatches 100% of Trt patients (unlike greedy caliper match)\nAttempts to optimize the total distance between all pairs\nHowever, resulted in much worse covariate balance"
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#summary",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#summary",
    "title": "Causal inference with observational data",
    "section": "Summary",
    "text": "Summary\n\nGoal: estimate causal effect of exposure \\(X\\) on outcome \\(Y\\), using observational data\nPS matching balances measured confounders between exposure groups (similar to RCT)\n\n Intuitive: show covariates are balanced after matching, like RCT \n Of course, unmeasured confounders may remain \n\nIf estimating ATT (ATC), then need to match all Trt (Control) patients. If high % unmatched, consider other methods."
  },
  {
    "objectID": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#alternatives",
    "href": "presentations/causal-inference-matching/2023-09-24_causal_inference_matching.html#alternatives",
    "title": "Causal inference with observational data",
    "section": "Alternatives",
    "text": "Alternatives\n\nStratification on PS uses all patients, but does not work well with survival data (Austin 2014b, 2016)\nInverse probability weighting and regression adjustment are very flexible, but generally not accepted by FDA (unlike matching) (Lu 2019)\nCausal Random Forests look promising: https://grf-labs.github.io/grf/"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#overview",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#overview",
    "title": "Regression Diagnostics",
    "section": "Overview",
    "text": "Overview\n\nWhat is linear regression?\nAssumptions\nDiagnostics and remedies for failed assumptions"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#linear-regression",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#linear-regression",
    "title": "Regression Diagnostics",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\nContinuous response: \\(\\big\\{Y_i\\big\\}_{i=1}^N = (Y_1, ...., Y_N)\\)\nPredictors: \\(\\boldsymbol{X}_i = (X_{i1}, ..., X_{iP})\\)\n\n\n https://www.mathbootcamps.com/reading-scatterplots/ \n\nModel:\n\\[\\begin{equation}\nY_i = \\beta_0 + \\sum_{j=1}^P \\beta_j * X_{ij} + \\epsilon_i \\\\\n\\end{equation}\\]\nIndependent normal errors with constant variance:\n\\[\\begin{equation}\n\\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#ordinary-least-squares-ols",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#ordinary-least-squares-ols",
    "title": "Regression Diagnostics",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\nEstimate \\(\\hat{\\boldsymbol{\\beta}}\\) such that “sum of squared residuals” is minimized: \\(\\sum_{i=1}^n(y_i - \\hat{y})^2\\), where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\sum_{j=1}^P \\hat{\\beta}_j * X_{ij}\\)\n\n\n\n\n\n\n https://medium.com/analytics-vidhya/ordinary-least-square-ols-method-for-linear-regression-ef8ca10aadfc"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#assumptions",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#assumptions",
    "title": "Regression Diagnostics",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nLinearity: relationship btwn \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) is approximately linear\nNormally distributed residuals\n\nOR the sample size is large (Central Limit Theorem)\n\n\n…simulations studies show that “sufficiently large” is often under 100, and even for our extremely nonNormal medical cost data it is less than 500. (Lumley et al. 2002)\n\nHomoscedasticity (equal variance): the residuals have equal variance at every value of \\(\\boldsymbol{X}\\)\nIndependence: residuals are independent (not correlated)\n\nOther issues:\n\nMulticollinearity: highly correlated predictors can greatly increase Var(\\(\\hat{\\beta}\\))\nInfluencial observations/outliers can bias results\nAdditivity: by default, assumes no interactions btwn predictors. Need to manually add interaction terms."
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#example-dataset",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#example-dataset",
    "title": "Regression Diagnostics",
    "section": "Example dataset",
    "text": "Example dataset\n\nOutcome = forced expiatory volume in liters (fev)\nN = 654 youths age 3-19 from East Boston during 1970’s\n\n\n\nGGally::ggpairs(fev, showStrips = TRUE)"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#initial-model-with-all-variables",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#initial-model-with-all-variables",
    "title": "Regression Diagnostics",
    "section": "Initial model with all variables",
    "text": "Initial model with all variables\n\n\nfit &lt;- lm(formula = fev ~., data = fev)\nsjPlot::tab_model(fit, ci.hyphen = ',&nbsp;', p.style = 'scientific', digits.p = 2);\n\n\n\n\n \nfev\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-4.46\n-4.89, -4.02\n1.07e-69\n\n\nage\n0.07\n0.05, 0.08\n1.21e-11\n\n\nheight inches\n0.10\n0.09, 0.11\n4.98e-80\n\n\nsex [Male]\n0.16\n0.09, 0.22\n2.74e-06\n\n\nsmoke [Yes]\n-0.09\n-0.20, 0.03\n1.41e-01\n\n\nObservations\n654\n\n\nR2 / R2 adjusted\n0.775 / 0.774"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#linearity",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#linearity",
    "title": "Regression Diagnostics",
    "section": "Linearity",
    "text": "Linearity\n\n\nPlot residuals (\\(y - \\hat{y}\\)) vs. each predictor and \\(\\hat{y}\\). Want to see horizontal band around 0 with no patterns.\nCurvature in the plots suggests non-linear relationship\n\n\n\n\ncar::residualPlots(fit);\n\n\n\n\n\n\n\n\n              Test stat Pr(&gt;|Test stat|)    \nage              5.0256        6.500e-07 ***\nheight.inches    7.6489        7.354e-14 ***\nsex                                         \nsmoke                                       \nTukey test       8.3559        &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ncar::residualPlots outputs a table which tests the linearity assumption of each continuous predictor. It reports the p-value for \\(X_j^2\\)."
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-non-linearity",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-non-linearity",
    "title": "Regression Diagnostics",
    "section": "Remedies for non-linearity",
    "text": "Remedies for non-linearity\n\nData transformation: transforming the outcome and/or predictor to make more normally distributed may help\nGAM: Generalized Additive Model automatically models linear/non-linear effects using smoothing splines: previous lab talks: 2022-12-08 and 2018-04-27\nPolynomials: e.g. \\(Age + Age^2 + Age^3 + ...\\)\n\nuse stats::poly() for uncorrelated polynomials; regular polynomials are usually highly correlated\n\n\n\n\nfit.poly &lt;- lm(fev ~ poly(age, 2) + poly(height.inches, 2) + sex + smoke, data = fev);\ncar::residualPlots(fit.poly, tests = FALSE);\n\n\n\n\n\n\n\n\n\n\nNote the adjusted \\(R^2\\) was 0.77 and 0.79 for the linear and polynomial models"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#normality-of-residuals-or-large-n",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#normality-of-residuals-or-large-n",
    "title": "Regression Diagnostics",
    "section": "Normality of residuals or large N",
    "text": "Normality of residuals or large N\n\nSimulations and refs from (Lumley et al. 2002) suggest our N = 654 is sufficient. Nevertheless, you can visually check normality below.\nP-value tests of Normality are not recommended: low power in the scenario you care about (small N) but usually significant in the scenario you don’t care about (large N)"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-non-normal-residuals",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-non-normal-residuals",
    "title": "Regression Diagnostics",
    "section": "Remedies for non-normal residuals",
    "text": "Remedies for non-normal residuals\n\nLarge \\(N\\)\nData transformation of outcome and/or predictors\nMake sure linearity assumption is met\nBootstrap confidence intervals for robust inference:\n\n\nset.seed(123);\nbootstrap &lt;- car::Boot(fit, method = 'case');\nround(confint(bootstrap), 3);\n\nBootstrap bca confidence intervals\n\n               2.5 % 97.5 %\n(Intercept)   -4.936 -3.940\nage            0.046  0.087\nheight.inches  0.093  0.114\nsexMale        0.103  0.231\nsmokeYes      -0.249  0.068"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#homoscedasticity-equal-variance",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#homoscedasticity-equal-variance",
    "title": "Regression Diagnostics",
    "section": "Homoscedasticity (equal variance)",
    "text": "Homoscedasticity (equal variance)\n\n\nPlot residuals vs fitted values: want flat/horizontal band around 0\nFunnel pattern like &lt; or &gt; indicates heteroscedasticity\n\n\n\npar(mfrow = c(1,2))\ncar::residualPlot(fit, main = 'Linear fit');\ncar::residualPlot(fit.poly, main = 'Polynomial fit');\n\n\n\n\n\ncar::spreadLevelPlot: \\(log(|\\text{studentized residuals}|)\\) vs. \\(log(\\hat{y})\\)\n\nFlat horizontal line means equal variance\n\n\n\n\nSuggested power transformation:  0.3772182 \n\n\n\n\n\n\nSuggested power transformation:  -0.09245971 \n\n\n\ncar::ncvTest(fit);\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 93.4299, Df = 1, p = &lt; 2.22e-16\n\ncar::ncvTest(fit.poly);\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 127.954, Df = 1, p = &lt; 2.22e-16"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-unequal-variance",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-unequal-variance",
    "title": "Regression Diagnostics",
    "section": "Remedies for unequal variance",
    "text": "Remedies for unequal variance\n\nTransform \\(\\boldsymbol{Y}\\): car::spreadLevelPlot() prints a “Suggested power transformation” \\(\\tau\\). Refit model with \\(\\boldsymbol{Y}^\\tau\\).\nUnequal variance affects \\(Var({\\hat{\\beta}})\\) but not \\(\\hat{\\beta}\\). Thus, can use robust standard errors or bootstrap for CIs/pvalues:\n\n\n# robust standard errors\nrobust.se &lt;- lmtest::coeftest(\n    x = fit, \n    vcov = sandwich::vcovHC(fit)\n    );\nconfint(robust.se);"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#independent-residuals",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#independent-residuals",
    "title": "Regression Diagnostics",
    "section": "Independent residuals",
    "text": "Independent residuals\n\nCorrelated residuals can occur from repeated measures, or when patients cluster by some group (e.g. family, hospital)\nPlot residuals vs time or other suspected clustering variables\nRemedies: robust sandwich standard errors to account for cluster effect; or linear mixed models"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#multicollinearity",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#multicollinearity",
    "title": "Regression Diagnostics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nHighly correlated predictors can increase \\(Var(\\hat{\\beta})\\), producing unreliable results\ncaret::findCorrelation removes predictors with corr &gt; cutoff\nVariance inflation factors: \\(\\text{VIF}(X_j) = \\frac{1}{1 - R^2_j}\\), where \\(R^2_j\\) is % variance of \\(X_j\\) explained by all other predictors\n\nVIF &gt; 10 is a common cutoff\n\n\n\ncar::vif(fit);\n\n          age height.inches           sex         smoke \n     3.019010      2.829728      1.060228      1.209564 \n\n\n\nOther remedies include PCA and regularized regression"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#influential-observations",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#influential-observations",
    "title": "Regression Diagnostics",
    "section": "Influential observations",
    "text": "Influential observations\n\nExtreme values in \\(\\boldsymbol{Y}\\) and/or \\(\\boldsymbol{X}\\) can highly influence results\n\\(|\\text{Standardized residuals}| &gt; 3\\) indicates potential outlier (If normally distributed, expect 99.7% &lt; 3)\n\n\n\nplot(\n    x = fitted(fit),\n    y = rstandard(fit),\n    ylab = 'Standardized Residuals',\n    xlab = 'Fitted values'\n    );\nabline(h = -3, lty = 2);\nabline(h = 3, lty = 2);"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#cooks-distance",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#cooks-distance",
    "title": "Regression Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\n\\(D_i = \\frac{\\sum_j(\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) * \\hat{\\sigma}^2}\\)\n\\(D_i\\) is proportional to the distance that the predicted values would move if the \\(i\\)th patient was excluded\nVarious cutoffs have been used in the literature, e.g. 1, \\(\\frac{4}{N}\\), or \\(\\frac{4}{N - p - 1}\\).. or visually identify patients with relatively large vals\n\n\n\n\ncar::influenceIndexPlot(fit, vars = 'Cook');"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-influential-observations",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#remedies-for-influential-observations",
    "title": "Regression Diagnostics",
    "section": "Remedies for influential observations",
    "text": "Remedies for influential observations\n\nSensitivity analysis: fit 2 models that include or exclude influential obs. Do the results substantially change?\nRobust regression: include all patients but downweight influential observations\n\nR: https://www.john-fox.ca/Companion/appendices/Appendix-Robust-Regression.pdf\nquantreg::rq() for median regression"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#interactions",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#interactions",
    "title": "Regression Diagnostics",
    "section": "Interactions",
    "text": "Interactions\n\nSomeone should do a short talk on interactions!\nBy default, linear regression assumes no interactions between predictors.\nYou can manually add interaction terms to the model to investigate. A*B in R formula gives A + B + A:B\n\n\n\n# allow all variables to interact with Sex\nfit.interaction &lt;- lm(fev ~ (age + height.inches + smoke) * sex, data = fev);\nsjPlot::tab_model(fit.interaction, ci.hyphen = ',&nbsp;', p.style = 'scientific', digits.p = 2);\n\n\n\n\n\n\n\n\n\n\n \nfev\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-3.36\n-4.07, -2.65\n1.93e-19\n\n\nage\n0.06\n0.03, 0.08\n5.05e-06\n\n\nheight inches\n0.09\n0.07, 0.10\n7.83e-30\n\n\nsmoke [Yes]\n-0.07\n-0.22, 0.08\n3.75e-01\n\n\nsex [Male]\n-1.32\n-2.23, -0.41\n4.48e-03\n\n\nage × sex [Male]\n0.03\n-0.01, 0.06\n1.39e-01\n\n\nheight inches × sex\n[Male]\n0.02\n0.00, 0.04\n4.07e-02\n\n\nsmoke [Yes] × sex [Male]\n0.02\n-0.22, 0.25\n8.93e-01\n\n\nObservations\n654\n\n\nR2 / R2 adjusted\n0.786 / 0.783\n\n\n\n\n\n\n\n\n\nIf you suspect many interactions, might be better to use a machine learning model that automatically handles interactions like decision-trees or Random Forest"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#summary",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#summary",
    "title": "Regression Diagnostics",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\nAssumption \nAssessment \n\nSolution \n\n\n\n\nLinearity\ncar::residualPlots, want horizontal band around 0 for each predictor\n\n- Transform \\(Y\\) or \\(X\\) - GAM to automate linear/non-linear - Polynomials\n\n\nNormality of residuals\n- Histogram/density plot - Normal QQ plot\n\n- Large N - Transform \\(Y\\) or \\(X\\) - Make sure linear assumption met - Bootstrap CIs: confint(car::Boot(fit))\n\n\nEqual variance\n- car::residualPlot and car::spreadLevelPlot, want horizontal band around 0 - car::ncvTest\n\n- Transform \\(Y\\) using exponent from spreadLevelPlot - Robust standard errors or bootstrap CI\n\n\nIndependent residuals\nPlot residuals vs time or other suspected clustering variables\n\n- Robust sandwich standard errors for cluster effect - Linear mixed models\n\n\nMulticollinearity\n- Check correlation between predictors - car::vif, want VIF &lt; 10\n\n- Given 2 highly corr. predictors, only keep 1 - caret::findCorrelation to remove predictors with corr &gt; cutoff - PCA, regularized regression\n\n\nInfluential obs\n- Plot standardized residuals vs fitted values; |r| &gt; 3 outlier - Cook’s distance, car::influenceIndexPlot\n\n- Sensitivity analysis fitting models with/without influential obs - Robust regression to downweight influential obs: quantreg::rq\n\n\nInteractions\n- Manually add interaction terms, significant?\n\n- Manually add interaction terms - Stratify model by potential interaction terms - ML models that automatically handle interactions"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#extensions-to-general-linear-models",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#extensions-to-general-linear-models",
    "title": "Regression Diagnostics",
    "section": "Extensions to General Linear Models",
    "text": "Extensions to General Linear Models\n\nGeneral linear model (GLM): for binary, multi-category, ordinal, or count outcomes\ncar::residualPlots to assess linearity of each predictor. Want to see horizontal band around 0 with no patterns. For non-linearity, use polynomials or GAM.\ncar::vif to assess multicollinearity\ncar::influenceIndexPlot to assess influential observations\ncar::Boot for robust bootstrap confidence intervals\nGLM also makes assumptions about the variance.. if false, can use bootstrap or robust standard errors:\n\n\nlmtest::coeftest(fit, vcov = sandwich::vcovHC(fit))"
  },
  {
    "objectID": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#references",
    "href": "presentations/regression-diagnostics/2023-09-24_regression_diagnostics.html#references",
    "title": "Regression Diagnostics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nLumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002. “The Importance of the Normality Assumption in Large Public Health Data Sets.” Annual Review of Public Health 23 (1): 151–69."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n1. Lippitt, Carlson, Arbet, Fingerlin, Maier, & Kechris. (2024). Limitations of clustering with PCA and correlated noise. Journal of Statistical Computation and Simulation. https://doi.org/10.1080/00949655.2024.2329976\n\n\n2. Ni, Rogowitz, Farahmand, Kaizer, Arbet, Cunningham, Thomas, & Saxon. (2024). Weight loss outcomes in a veterans affairs pharmacotherapy-based weight management clinic. Journal of the Endocrine Society. https://doi.org/10.1210/jendso/bvae042\n\n\n3. Tohi, Sahrmann, Arbet, Kato, Lee, Peacock, Ginsburg, Pavlovich, Carroll, Bangma, Sugimoto, & Boutros. (2024). De-escalation of monitoring in active surveillance for prostate cancer: Results from the GAP3 consortium. European Urology Oncology. https://doi.org/10.1016/j.euo.2024.07.006\n\n\n4. Weiner, Agrawal, Wang, Sonni, Li, Arbet, Zhang, Proudfoot, Hong, Davicioni, Kane, Valle, Kishan, Pra, Ghadjar, Sweeney, Nickols, Karnes, Shen, Rettig, Czernin, Ross, Chua, L. K., Schaeffer, Calais, Boutros, & Reiter. (2024). Molecular hallmarks of prostate-specific membrane antigen in treatment-na&lt;u+00EF&gt;ve prostate cancer. European Urology. https://doi.org/10.1016/j.eururo.2024.09.005\n\n\n5. Chan, Dodson, Arbet, Boutros, & Xiao. (2023). Single-cell analysis in lung adenocarcinoma implicates RNA editing in cancer innate immunity and patient prognosis. Cancer Research. https://doi.org/10.1158/0008-5472.CAN-22-1062\n\n\n6. Gamallat, Choudhry, Li, Rokne, Alhajj, Abdelsalam, Ghosh, Arbet, Boutros, & Bismar. (2023). Serrate RNA effector molecule (SRRT) is associated with prostate cancer progression and is a predictor of poor prognosis in lethal prostate cancer. Cancers. https://doi.org/10.3390/cancers15102867\n\n\n7. Greca, Grau, Arbet, Liao, Sosa, Haugen, & Kitahara. (2023). Anthropometric, dietary, and lifestyle factors and risk of advanced thyroid cancer: The NIH-AARP diet and health cohort study. Clinical Endocrinology. https://doi.org/10.1111/cen.14970\n\n\n8. Creasy, Ostendorf, Blankenship, Grau, Arbet, Bessesen, Melanson, & Catenacci. (2022). Effect of sleep on weight loss and adherence to diet and physical activity recommendations during an 18-month behavioral weight loss intervention. International Journal of Obesity (2005). https://doi.org/10.1038/s41366-022-01141-z\n\n\n9. Grau, Arbet, Ostendorf, Blankenship, Panter, Catenacci, Melanson, & Creasy. (2022). Creating an algorithm to identify indices of sleep quantity and quality from a wearable armband in adults. Sleep Science (Sao Paulo, Brazil). https://doi.org/10.5935/1984-0063.20220052\n\n\n10. Lin, Arbet, Mroz, Liao, Restrepo, Mayer, Li, Barkes, Schrock, Hamzeh, Fingerlin, Carlson, & Maier. (2022). Clinical phenotyping in sarcoidosis using cluster analysis. Respiratory Research. https://doi.org/10.1186/s12931-022-01993-z\n\n\n11. Okamoto, Devoe, Seto, Minarchick, Wilson, Rothfuss, Mohning, Arbet, Kroehl, Visser, August, Thomas, Charry, Fleischer, Feser, Frazer-Abel, Norris, Cherrington, Janssen, Kaplan, Deane, Holers, & Demoruelle. (2022). Association of sputum neutrophil extracellular trap subsets with IgA anti-citrullinated protein antibodies in subjects at risk for rheumatoid arthritis. Arthritis & Rheumatology (Hoboken, N.J.). https://doi.org/10.1002/art.41948\n\n\n12. Wood, Arbet, Amura, Nodine, Collins, Orlando, Mayer, Stein, & Anderson. (2022). Multicenter study evaluating nitrous oxide use for labor analgesia at high- and low-altitude institutions. Anesthesia and Analgesia. https://doi.org/10.1213/ANE.0000000000005712\n\n\n13. Arbet, Zhuang, Litkowski, Saba, & Kechris. (2021). Comparing statistical tests for differential network analysis of gene modules. Frontiers in Genetics. https://doi.org/10.3389/fgene.2021.630215\n\n\n14. Carpenter, Frank, Williamson, Arbet, Wagner, Kechris, & Kroehl. (2021). tidyMicro: A pipeline for microbiome data analysis and visualization using the tidyverse in r. BMC Bioinformatics. https://doi.org/10.1186/s12859-021-03967-2\n\n\n15. Nodine, Arbet, Jenkins, Rosenthal, Carrington, Purcell, Lee, & Hoon. (2021). Graduate nursing student stressors during the COVID-19 pandemic. Journal of Professional Nursing : Official Journal of the American Association of Colleges of Nursing. https://doi.org/10.1016/j.profnurs.2021.04.008\n\n\n16. Ostendorf, Blankenship, Grau, Arbet, Mitchell, Creasy, Caldwell, Melanson, Phelan, Bessesen, & Catenacci. (2021). Predictors of long-term weight loss trajectories during a behavioral weight loss intervention: An exploratory analysis. Obesity Science & Practice. https://doi.org/10.1002/osp4.530\n\n\n17. Ramakrishnan, Arbet, Mace, Suresh, Smith, S., Soler, & Smith. (2021). Predicting olfactory loss in chronic rhinosinusitis using machine learning. Chemical Senses. https://doi.org/10.1093/chemse/bjab042\n\n\n18. Reed, Arbet, & Staubli. (2021). Clinical nurse specialists in the united states registered with a national provider identifier. Clinical Nurse Specialist CNS. https://doi.org/10.1097/NUR.0000000000000592\n\n\n19. Rosenthal, Lee, Jenkins, Arbet, Carrington, Hoon, Purcell, & Nodine. (2021). A survey of mental health in graduate nursing students during the COVID-19 pandemic. Nurse Educator. https://doi.org/10.1097/NNE.0000000000001013\n\n\n20. Schmanski, Roberts, Coors, Wicks, Arbet, Weber, Crooks, Barnes, & Taylor. (2021). Research participant understanding and engagement in an institutional, self-consent biobank model. Journal of Genetic Counseling. https://doi.org/10.1002/jgc4.1316\n\n\n21. Arbet, Brokamp, Meinzen-Derr, Trinkley, & Spratt. (2020). Lessons and tips for designing a machine learning study using EHR data. Journal of Clinical and Translational Science. https://doi.org/10.1017/cts.2020.513\n\n\n22. Arbet, McGue, & Basu. (2020). A robust and unified framework for estimating heritability in twin studies using generalized estimating equations. Statistics in Medicine. https://doi.org/10.1002/sim.8564\n\n\n23. Coleman-Minahan, Sheeder, Arbet, & McLemore. (2020). Interest in medication and aspiration abortion training among colorado nurse practitioners, nurse midwives, and physician assistants. Women’s Health Issues : Official Publication of the Jacobs Institute of Women’s Health. https://doi.org/10.1016/j.whi.2020.02.001\n\n\n24. Gance-Cleveland, Linton, Arbet, Stiller, & Sylvain. (2020). Predictors of overweight and obesity in childhood cancer survivors. Journal of Pediatric Oncology Nursing : Official Journal of the Association of Pediatric Oncology Nurses. https://doi.org/10.1177/1043454219897102\n\n\n25. Thomas, Zaman, Cornier, Catenacci, Tussey, Grau, Arbet, Broussard, & Rynders. (2020). Later meal and sleep timing predicts higher percent body fat. Nutrients. https://doi.org/10.3390/nu13010073\n\n\n26. James-Allan, Arbet, Teal, Powell, & Jansson. (2019). Insulin stimulates GLUT4 trafficking to the syncytiotrophoblast basal plasma membrane in the human placenta. The Journal of Clinical Endocrinology and Metabolism. https://doi.org/10.1210/jc.2018-02778\n\n\n27. Arbet, McGue, Chatterjee, & Basu. (2017). Resampling-based tests for lasso in genome-wide association studies. BMC Genetics. https://doi.org/10.1186/s12863-017-0533-3\n\n\n28. Grinde, Arbet, Green, O’Connell, Valcarcel, Westra, & Tintle. (2017). Illustrating, quantifying, and correcting for bias in post-hoc analysis of gene-based rare variant tests of association. Frontiers in Genetics. https://doi.org/10.3389/fgene.2017.00117\n\n\n29. Greco, Hainline, Arbet, Grinde, Benitez, & Tintle. (2016). A general approach for combining diverse rare variant association tests provides improved robustness across a wider range of genetic architectures. European Journal of Human Genetics : EJHG. https://doi.org/10.1038/ejhg.2015.194"
  },
  {
    "objectID": "presentations/MARS/mars.html",
    "href": "presentations/MARS/mars.html",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "",
    "text": "library(BoutrosLab.plotting.general);\n\nLoading required package: lattice\n\n\nLoading required package: latticeExtra\n\n\nLoading required package: cluster\n\n\nLoading required package: hexbin\n\n\nWarning: package 'hexbin' was built under R version 4.1.3\n\n\nLoading required package: grid\n\n\n\nAttaching package: 'BoutrosLab.plotting.general'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\nlibrary(GGally);\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:latticeExtra':\n\n    layer\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(earth);\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nLoading required package: plotrix\n\n\nLoading required package: TeachingDemos\n\nlibrary(pdp);\nseed &lt;- 1234;\n\nsource('utilities.R');\ndata(PreDiabetes, package = 'MLDataR');\n\ndiabetes &lt;- data.frame(subset(x = PreDiabetes, select = c(Time_Pre_To_Diabetes, Sex, IMD_Decile, BMI, Age_PreDiabetes, HbA1C, PreDiabetes_Checks_Before_Diabetes)));\n\ncolnames(diabetes)[colnames(diabetes) == 'Time_Pre_To_Diabetes'] &lt;- 'Years_Pre_To_Diabetes';\ndata(fev, package = 'mplot');\ncolnames(fev)[colnames(fev) == 'height'] &lt;- 'height.inches';\nfev$sex &lt;- factor(fev$sex, levels = c(0,1), labels = c('Female', 'Male'));\nfev$smoke &lt;- factor(fev$smoke, levels = c(0, 1), labels = c('No', 'Yes'));"
  },
  {
    "objectID": "presentations/MARS/mars.html#motivation",
    "href": "presentations/MARS/mars.html#motivation",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Motivation",
    "text": "Motivation\nGoal: build a flexible yet interpretable model like:\n\\[\\begin{equation}\n\\boldsymbol{y} = f(\\boldsymbol{x}_1, ..., \\boldsymbol{x}_P) + \\boldsymbol{e}\n\\end{equation}\\]\nGeneral linear models (GLM), e.g. linear or logistic regression are interpretable and can be flexible, but you need to decide:\n\nWhich predictors to include?\nFor each predictor: linear or non-linear effect?\nDo predictors interact? If yes, 2-way, 3-way, …?\n\n\n\n\n\n\n\n\nMARS (multivariate adaptive regression splines) automatically determines all of this for you 😃"
  },
  {
    "objectID": "presentations/MARS/mars.html#piecewise-linear-functions",
    "href": "presentations/MARS/mars.html#piecewise-linear-functions",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Piecewise linear functions",
    "text": "Piecewise linear functions\nMARS uses simple piecewise linear functions (“splines”) that can approximate complex relationships\n\nKnots: points in \\(\\boldsymbol{X}\\) where effect on \\(\\boldsymbol{Y}\\) (slope) changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: what are 3 things you need to determine when fitting a piecewise linear spline?\n\n\nNumber of knots\nLocation of knots\nHow the slopes change at each knot (i.e. estimate separate slopes within each interval)\n\n\nMARS automatically decides all 3!\n\n\n\n\n\nEveringham, Y. L., J. Sexton, and Jimmy White. “An introduction to multivariate adaptive regression splines for the cane industry.” Proceedings of the 2011 Conference of the Australian Society of Sugar Cane Technologists. 2011.\nhttps://jekel.me/2017/Fit-a-piecewise-linear-function-to-data/\nhttps://flexbooks.ck12.org/cbook/ck-12-interactive-algebra-1-for-ccss/section/1.4/primary/lesson/piecewise-linear-functions-alg-1-ccss/"
  },
  {
    "objectID": "presentations/MARS/mars.html#hinge-function",
    "href": "presentations/MARS/mars.html#hinge-function",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Hinge function",
    "text": "Hinge function\n\nMain building block of MARS used to construct the piecewise linear functions\nFor predictor \\(x\\) and knot at \\(x=t\\), hinge fn has 2 parts (Hastie et al. 2009):\n\n\n\n\n\n\n\n\n\n\n\n\nThe hinge fn comes in a pair of terms: Left \\((t-x)_+\\) and Right \\((x-t)_+\\)\nIf MARS selects a given hinge fn, it is input to a GLM and estimates 2 coefficients: \\(\\beta_{Left}(t-x)_+ + \\beta_{Right}(x-t)\\)"
  },
  {
    "objectID": "presentations/MARS/mars.html#section",
    "href": "presentations/MARS/mars.html#section",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "",
    "text": "Suppose knot \\(t = 0.5\\). Here is what the hinge \\(\\beta_{L}(t-x)_+ + \\beta_{R}(x-t)\\) looks like for various coeffients:"
  },
  {
    "objectID": "presentations/MARS/mars.html#why-piecewise-linear-hinge-functions",
    "href": "presentations/MARS/mars.html#why-piecewise-linear-hinge-functions",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Why piecewise linear hinge functions?",
    "text": "Why piecewise linear hinge functions?\n\nMany fancier splines exist (e.g. cubic, smoothing, penalized, B-splines)\n\n\n\nThe piecewise linear hinges are generally much faster and easier to implement for big data\n\n…the regression surface is built up parsimoniously, using nonzero components locally—only where they are needed. This is important, since one should “spend” parameters carefully in high dimensions, as they can run out quickly [“curse of dimensionality”]. The use of other basis functions such as polynomials, would produce a nonzero product everywhere, and would not work as well (Hastie et al. 2009)"
  },
  {
    "objectID": "presentations/MARS/mars.html#overview",
    "href": "presentations/MARS/mars.html#overview",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Overview",
    "text": "Overview\n\nForward: build a large # hinges that overfit the data\nBackward: use backward VS to prune the model\nEstimate the final coefficients in lm/glm\n\n\n  http://www.milbo.org/doc/earth-notes.pdf\nNonparametric?\n\nParametric models require the user to pre-determine all parameters of the model to be estimated.\nNonparametric models like MARS automatically determine how to parameterize the model (# and form); more flexible."
  },
  {
    "objectID": "presentations/MARS/mars.html#forward-step",
    "href": "presentations/MARS/mars.html#forward-step",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Forward step",
    "text": "Forward step\n\n\\(\\boldsymbol{M}\\) = set of terms in model. Start with just an intercept \\(\\{1\\}\\).\n\\(\\boldsymbol{C}\\) = set of candidate hinge functions to add to model. Contains hinge functions at each observed value for each predictor (\\(N * P * 2\\) total terms):\n\n\\[\\begin{equation}\n\\boldsymbol{C} = \\big\\{(X_j - t)_+, (t - X_j)_+\\big\\} \\\\ t \\in \\{x_{1j}, x_{2j}, ..., x_{Nj}\\}; \\ j = 1,2, ..., P\n\\end{equation}\\]\n\n“At each stage we consider all products of a candidate hinge in \\(\\boldsymbol{C}\\) with a hinge in the model \\(\\boldsymbol{M}\\). The product that decreases the residual error the most is added into the current model.” (Hastie et al. 2009)\n\nThus at each step, it’s possible to add:\n\nInteraction term\nNew variable\nNew knot to an existing variable in the model\n\nExample first 3 steps:\n\n\n\n\n\n\n\n\nStop once maximum number of terms is reached"
  },
  {
    "objectID": "presentations/MARS/mars.html#backwards-step",
    "href": "presentations/MARS/mars.html#backwards-step",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Backwards step",
    "text": "Backwards step\n\nForward step purposely builds a large model that overfits\nBackward step prunes the model to reduce overfitting:\n\n\nThe term whose removal causes the smallest increase in residual squared error is deleted from the model at each stage, producing an estimated best model \\(f_\\lambda\\) of each size (number of terms) \\(λ\\) (Hastie et al. 2009)\n\nThus the best models of size 1, 2, …, nprune features are identified\nBest? Measured by fast generalized cross validation (GCV) or more accurate but slower K-fold CV\n\nGCV provides a convenient approximation to leave-one out cross-validation for linear models [without needing to split/resample/refit data](Hastie et al. 2009)"
  },
  {
    "objectID": "presentations/MARS/mars.html#tuning-mars",
    "href": "presentations/MARS/mars.html#tuning-mars",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Tuning MARS",
    "text": "Tuning MARS\nPotential tuning parameters:\n\nMax degree of interactions allowed (set to 1 for none).\nNumber of retained terms in the final model (nprune)\nMax number of terms in the Forward step (nk)\n\nSimplest tuning strategy:\n\nSet degree to a moderate value like 5 and use default nk\n\nGCV is used to automatically select nprune\nIf “Reached max number of terms” then increase nk\nIf many 5-way interactions, then increase degree\n\n\nMedium tuning strategy:\n\nSame as above but use K-fold CV to select nprune\n\nAdvanced tuning strategy:\n\nGrid for degree, nk and use K-fold CV to optimize."
  },
  {
    "objectID": "presentations/MARS/mars.html#example",
    "href": "presentations/MARS/mars.html#example",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Example",
    "text": "Example\n\nOutcome = forced expiatory volume in liters (fev)\nN = 654 youths age 3-19 from East Boston during 1970’s\n\n\n\nGGally::ggpairs(fev, showStrips = TRUE)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "presentations/MARS/mars.html#fitting-mars-with-earth-r-package",
    "href": "presentations/MARS/mars.html#fitting-mars-with-earth-r-package",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Fitting MARS with earth R package",
    "text": "Fitting MARS with earth R package\n\nBy default, uses GCV to select optimal number of terms\n\n\nlibrary(earth);\nfit.gcv &lt;- earth(\n    formula = fev ~.,\n    data = fev,\n    degree = 5,\n    keepxy = TRUE\n    );\nprint(fit.gcv);\n\nSelected 6 of 17 terms, and 4 of 4 predictors\nTermination condition: Reached nk 21\nImportance: height.inches, sexMale, age, smokeYes\nNumber of terms at each degree of interaction: 1 3 2\nGCV 0.1487769    RSS 93.32458    GRSq 0.8024061    RSq 0.8098985\n\n\n\nSelected all 4 predictors, using 6 total hinge functions\nGRSq normalizes GCV from 0 to 1, similar to adjusted \\(R^2\\).\n\n\n\nplot(fit.gcv, which = c(1));"
  },
  {
    "objectID": "presentations/MARS/mars.html#using-10-fold-cv-5-repeats-to-select-the-model-size",
    "href": "presentations/MARS/mars.html#using-10-fold-cv-5-repeats-to-select-the-model-size",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Using 10-fold CV (5 repeats) to select the model size",
    "text": "Using 10-fold CV (5 repeats) to select the model size\n\n\nset.seed(123);\nfit.cv &lt;- earth(\n    formula = fev ~.,\n    data = fev,\n    degree = 5,\n    keepxy = TRUE,\n    pmethod = 'cv',\n    nfold = 10,\n    ncross = 5\n    );\nprint(fit.cv);\n\nSelected 5 of 17 terms, and 3 of 4 predictors (pmethod=\"cv\")\nTermination condition: Reached nk 21\nImportance: height.inches, sexMale, age, smokeYes-unused\nNumber of terms at each degree of interaction: 1 3 1\nGRSq 0.8013863  RSq 0.8074228  mean.oof.RSq 0.7912661 (sd 0.049)\n\npmethod=\"backward\" would have selected:\n    6 terms 4 preds,  GRSq 0.8024061  RSq 0.8098985  mean.oof.RSq 0.7878347\n\nplot(fit.cv, which = 1);\n\n\n\n\n\n\nVery similar GRSq/RSq as using the simpler GCV tuning method, but 1 less term and omits smoke status"
  },
  {
    "objectID": "presentations/MARS/mars.html#predictor-effects",
    "href": "presentations/MARS/mars.html#predictor-effects",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Predictor effects",
    "text": "Predictor effects\nLet’s explore the predictor effects from fit.gcv:\nSelected hinge functions and \\(\\hat{\\beta}\\):\n\nfit.gcv$coefficients;\n\n                                    fev\n(Intercept)                  2.74759130\nh(65-height.inches)         -0.09188745\nh(age-8)                     0.08327821\nh(height.inches-65)*sexMale  0.24942931\nh(height.inches-68)         -0.14391813\nh(age-8)*smokeYes           -0.02834601\n\n\nVariable importance scores:\n\n\nevimp(fit.gcv);\n\n              nsubsets   gcv    rss\nheight.inches        5 100.0  100.0\nsexMale              4  46.1   46.5\nage                  3  16.5   17.9\nsmokeYes             1   3.6    5.5\n\n\n\nPartial dependence plots:\n\nplotmo() can be used to plot the estimated effects\nI prefer the pdp R package for making similar plots:\n\n\n\nplot.features &lt;- list(\n    c('height.inches'),\n    c('age'),\n    c('sex'),\n    c('smoke'),\n    c('height.inches', 'sex'),\n    c('age', 'smoke')\n    );\npdps &lt;- lapply(\n    X = plot.features,\n    FUN = function(x) {\n        title &lt;- ifelse(length(x) == 1, x, paste(x, collapse = ', '));\n        p &lt;- pdp::partial(\n            object = fit.gcv,\n            pred.var = x,\n            rug = T\n            );\n        pdp::plotPartial(p, main = title, ylim = c(1, 4),  rug = TRUE, train = fit.gcv$data);\n        }\n    );\n\ncreate.multipanelplot(\n    filename = './figures/pdps.png',\n    resolution = 500,\n    plot.objects = pdps,\n    layout.width = 3,\n    layout.height = 2,\n    width = 10,\n    height = 7\n    );"
  },
  {
    "objectID": "presentations/MARS/mars.html#extensions",
    "href": "presentations/MARS/mars.html#extensions",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Extensions",
    "text": "Extensions\n\nMARS/earth can be used for continuous, count, binary or multinomial outcomes\n\nIf multinomial, it’s recommended to also try using MARS with Flexable Discriminant Analaysis (FDA) - see “Notes on the earth package” for details.\n\nIn theory, MARS can handle missing values and time-to-event outcomes. However, I’m not aware of any R implementations that support this.\ncaret’s bagged MARS can improve prediction performance at the expense of interpretability"
  },
  {
    "objectID": "presentations/MARS/mars.html#summary",
    "href": "presentations/MARS/mars.html#summary",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Summary",
    "text": "Summary\nMARS automatically handles:\n\nFeature engineering: what type of features to include? linear or non-linear, additive or interaction?\nFeature selection: given a high-dimensional set of initial features, which should you include?\n\nOther benefits:\n\nFast\nEasy to tune: simplest approach has 0 tuning parameters\nInterpretable\nHandles mixed numeric/categorical predictors without needing further transformation (similar to tree-methods)\nRobust to outliers in the predictors\n\nDownsides?\n\n\nIn my experience, although MARS is more interpretable, it generally has lower predictive performance compared to Random Forests. Bagging and/or random variable sets (like RF) might improve this, but needs further investigation.\nNo statistical inference (p-values or confidence intervals). I have ideas for how to do this, talk with me if interested.\nAlthough MARS in theory can handle missing values or time-to-event outcomes, I’m not aware of any free software that supports this. In contrast, many packages for tree-based models supports missing values and time-to-event outcomes (e.g. randomForestSRC R package)."
  },
  {
    "objectID": "presentations/MARS/mars.html#questions",
    "href": "presentations/MARS/mars.html#questions",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "presentations/MARS/mars.html#references",
    "href": "presentations/MARS/mars.html#references",
    "title": "Nonparametric feature selection for big data with MARS",
    "section": "References",
    "text": "References\n\n“Notes on the earth package”\nOriginal MARS paper: Friedman, Jerome H. “Multivariate adaptive regression splines.” The annals of statistics 19.1 (1991): 1-67.\n\nA slightly more accessible Intro: Friedman, Jerome H., and Charles B. Roosen. “An introduction to multivariate adaptive regression splines.” Statistical methods in medical research 4.3 (1995): 197-217.\n\nIMO, the Elements of Statistical Learning chapter on MARS is the best short introduction"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html",
    "title": "Regression Diagnostics",
    "section": "",
    "text": "library(BoutrosLab.plotting.general);\n\nLoading required package: lattice\n\n\nLoading required package: latticeExtra\n\n\nLoading required package: cluster\n\n\nLoading required package: hexbin\n\n\nWarning: package 'hexbin' was built under R version 4.1.3\n\n\nLoading required package: grid\n\n\n\nAttaching package: 'BoutrosLab.plotting.general'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\nlibrary(mplot);\n\nWarning: package 'mplot' was built under R version 4.1.3\n\nlibrary(GGally);\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:latticeExtra':\n\n    layer\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(sjPlot);\n\nLearn more about sjPlot with 'browseVignettes(\"sjPlot\")'.\n\nlibrary(car);\n\nLoading required package: carData\n\nseed &lt;- 1234;\n\nsource('utilities.R');\ndata(fev, package = 'mplot');\ncolnames(fev)[colnames(fev) == 'height'] &lt;- 'height.inches';\nfev$sex &lt;- factor(fev$sex, levels = c(0,1), labels = c('Female', 'Male'));\nfev$smoke &lt;- factor(fev$smoke, levels = c(0, 1), labels = c('No', 'Yes'));"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#overview",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#overview",
    "title": "Regression Diagnostics",
    "section": "Overview",
    "text": "Overview\n\nWhat is linear regression?\nAssumptions\nDiagnostics and remedies for failed assumptions"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#linear-regression",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#linear-regression",
    "title": "Regression Diagnostics",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\nContinuous response: \\(\\big\\{Y_i\\big\\}_{i=1}^N = (Y_1, ...., Y_N)\\)\nPredictors: \\(\\boldsymbol{X}_i = (X_{i1}, ..., X_{iP})\\)\n\n\n https://www.mathbootcamps.com/reading-scatterplots/ \n\nModel:\n\\[\\begin{equation}\nY_i = \\beta_0 + \\sum_{j=1}^P \\beta_j * X_{ij} + \\epsilon_i \\\\\n\\end{equation}\\]\nIndependent normal errors with constant variance:\n\\[\\begin{equation}\n\\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\end{equation}\\]"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#ordinary-least-squares-ols",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#ordinary-least-squares-ols",
    "title": "Regression Diagnostics",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\nEstimate \\(\\hat{\\boldsymbol{\\beta}}\\) such that “sum of squared residuals” is minimized: \\(\\sum_{i=1}^n(y_i - \\hat{y})^2\\), where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\sum_{j=1}^P \\hat{\\beta}_j * X_{ij}\\)\n\n\n\n\n\n\n https://medium.com/analytics-vidhya/ordinary-least-square-ols-method-for-linear-regression-ef8ca10aadfc"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#assumptions",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#assumptions",
    "title": "Regression Diagnostics",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nLinearity: relationship btwn \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) is approximately linear\nNormally distributed residuals\n\nOR the sample size is large (Central Limit Theorem)\n\n\n…simulations studies show that “sufficiently large” is often under 100, and even for our extremely nonNormal medical cost data it is less than 500. (Lumley et al. 2002)\n\nHomoscedasticity (equal variance): the residuals have equal variance at every value of \\(\\boldsymbol{X}\\)\nIndependence: residuals are independent (not correlated)\n\nOther issues:\n\nMulticollinearity: highly correlated predictors can greatly increase Var(\\(\\hat{\\beta}\\))\nInfluencial observations/outliers can bias results\nAdditivity: by default, assumes no interactions btwn predictors. Need to manually add interaction terms."
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#example-dataset",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#example-dataset",
    "title": "Regression Diagnostics",
    "section": "Example dataset",
    "text": "Example dataset\n\nOutcome = forced expiatory volume in liters (fev)\nN = 654 youths age 3-19 from East Boston during 1970’s\n\n\n\nGGally::ggpairs(fev, showStrips = TRUE)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#initial-model-with-all-variables",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#initial-model-with-all-variables",
    "title": "Regression Diagnostics",
    "section": "Initial model with all variables",
    "text": "Initial model with all variables\n\n\nfit &lt;- lm(formula = fev ~., data = fev)\nsjPlot::tab_model(fit, ci.hyphen = ',&nbsp;', p.style = 'scientific', digits.p = 2);\n\n\n\n\n \nfev\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-4.46\n-4.89, -4.02\n1.07e-69\n\n\nage\n0.07\n0.05, 0.08\n1.21e-11\n\n\nheight inches\n0.10\n0.09, 0.11\n4.98e-80\n\n\nsex [Male]\n0.16\n0.09, 0.22\n2.74e-06\n\n\nsmoke [Yes]\n-0.09\n-0.20, 0.03\n1.41e-01\n\n\nObservations\n654\n\n\nR2 / R2 adjusted\n0.775 / 0.774"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#linearity",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#linearity",
    "title": "Regression Diagnostics",
    "section": "Linearity",
    "text": "Linearity\n\n\nPlot residuals (\\(y - \\hat{y}\\)) vs. each predictor and \\(\\hat{y}\\). Want to see horizontal band around 0 with no patterns.\nCurvature in the plots suggests non-linear relationship\n\n\n\n\ncar::residualPlots(fit);\n\n\n\n\n\n\n\n\n              Test stat Pr(&gt;|Test stat|)    \nage              5.0256        6.500e-07 ***\nheight.inches    7.6489        7.354e-14 ***\nsex                                         \nsmoke                                       \nTukey test       8.3559        &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ncar::residualPlots outputs a table which tests the linearity assumption of each continuous predictor. It reports the p-value for \\(X_j^2\\)."
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-non-linearity",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-non-linearity",
    "title": "Regression Diagnostics",
    "section": "Remedies for non-linearity",
    "text": "Remedies for non-linearity\n\nData transformation: transforming the outcome and/or predictor to make more normally distributed may help\nGAM: Generalized Additive Model automatically models linear/non-linear effects using smoothing splines: previous lab talks: 2022-12-08 and 2018-04-27\nPolynomials: e.g. \\(Age + Age^2 + Age^3 + ...\\)\n\nuse stats::poly() for uncorrelated polynomials; regular polynomials are usually highly correlated\n\n\n\n\nfit.poly &lt;- lm(fev ~ poly(age, 2) + poly(height.inches, 2) + sex + smoke, data = fev);\ncar::residualPlots(fit.poly, tests = FALSE);\n\n\n\n\n\n\n\n\n\n\nNote the adjusted \\(R^2\\) was 0.77 and 0.79 for the linear and polynomial models"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#normality-of-residuals-or-large-n",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#normality-of-residuals-or-large-n",
    "title": "Regression Diagnostics",
    "section": "Normality of residuals or large N",
    "text": "Normality of residuals or large N\n\nSimulations and refs from (Lumley et al. 2002) suggest our N = 654 is sufficient. Nevertheless, you can visually check normality below.\nP-value tests of Normality are not recommended: low power in the scenario you care about (small N) but usually significant in the scenario you don’t care about (large N)\n\n\n\npar(mfrow = c(2, 2));\n\nhist(residuals(fit), main = 'Linear fit', xlim = c(-2, 2));\nhist(residuals(fit.poly), main = 'Polynomial fit', xlim = c(-2, 2));\nqqnorm(residuals(fit), main = 'Linear: Normal QQ plot');\nqqline(residuals(fit));\nqqnorm(residuals(fit.poly), main = 'Polynomial: Normal QQ plot');\nqqline(residuals(fit.poly));"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-non-normal-residuals",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-non-normal-residuals",
    "title": "Regression Diagnostics",
    "section": "Remedies for non-normal residuals",
    "text": "Remedies for non-normal residuals\n\nLarge \\(N\\)\nData transformation of outcome and/or predictors\nMake sure linearity assumption is met\nBootstrap confidence intervals for robust inference:\n\n\nset.seed(123);\nbootstrap &lt;- car::Boot(fit, method = 'case');\nround(confint(bootstrap), 3);\n\nBootstrap bca confidence intervals\n\n               2.5 % 97.5 %\n(Intercept)   -4.936 -3.940\nage            0.046  0.087\nheight.inches  0.093  0.114\nsexMale        0.103  0.231\nsmokeYes      -0.249  0.068"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#homoscedasticity-equal-variance",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#homoscedasticity-equal-variance",
    "title": "Regression Diagnostics",
    "section": "Homoscedasticity (equal variance)",
    "text": "Homoscedasticity (equal variance)\n\n\nPlot residuals vs fitted values: want flat/horizontal band around 0\nFunnel pattern like &lt; or &gt; indicates heteroscedasticity\n\n\n\npar(mfrow = c(1,2))\ncar::residualPlot(fit, main = 'Linear fit');\ncar::residualPlot(fit.poly, main = 'Polynomial fit');\n\n\n\n\n\ncar::spreadLevelPlot: \\(log(|\\text{studentized residuals}|)\\) vs. \\(log(\\hat{y})\\)\n\nFlat horizontal line means equal variance\n\n\n\n\nSuggested power transformation:  0.3772182 \n\n\n\n\n\n\nSuggested power transformation:  -0.09245971 \n\n\n\ncar::ncvTest(fit);\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 93.4299, Df = 1, p = &lt; 2.22e-16\n\ncar::ncvTest(fit.poly);\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 127.954, Df = 1, p = &lt; 2.22e-16"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-unequal-variance",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-unequal-variance",
    "title": "Regression Diagnostics",
    "section": "Remedies for unequal variance",
    "text": "Remedies for unequal variance\n\nTransform \\(\\boldsymbol{Y}\\): car::spreadLevelPlot() prints a “Suggested power transformation” \\(\\tau\\). Refit model with \\(\\boldsymbol{Y}^\\tau\\).\nUnequal variance affects \\(Var({\\hat{\\beta}})\\) but not \\(\\hat{\\beta}\\). Thus, can use robust standard errors or bootstrap for CIs/pvalues:\n\n\n# robust standard errors\nrobust.se &lt;- lmtest::coeftest(\n    x = fit, \n    vcov = sandwich::vcovHC(fit)\n    );\nconfint(robust.se);"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#independent-residuals",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#independent-residuals",
    "title": "Regression Diagnostics",
    "section": "Independent residuals",
    "text": "Independent residuals\n\nCorrelated residuals can occur from repeated measures, or when patients cluster by some group (e.g. family, hospital)\nPlot residuals vs time or other suspected clustering variables\nRemedies: robust sandwich standard errors to account for cluster effect; or linear mixed models"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#multicollinearity",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#multicollinearity",
    "title": "Regression Diagnostics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nHighly correlated predictors can increase \\(Var(\\hat{\\beta})\\), producing unreliable results\ncaret::findCorrelation removes predictors with corr &gt; cutoff\nVariance inflation factors: \\(\\text{VIF}(X_j) = \\frac{1}{1 - R^2_j}\\), where \\(R^2_j\\) is % variance of \\(X_j\\) explained by all other predictors\n\nVIF &gt; 10 is a common cutoff\n\n\n\ncar::vif(fit);\n\n          age height.inches           sex         smoke \n     3.019010      2.829728      1.060228      1.209564 \n\n\n\nOther remedies include PCA and regularized regression"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#influential-observations",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#influential-observations",
    "title": "Regression Diagnostics",
    "section": "Influential observations",
    "text": "Influential observations\n\nExtreme values in \\(\\boldsymbol{Y}\\) and/or \\(\\boldsymbol{X}\\) can highly influence results\n\\(|\\text{Standardized residuals}| &gt; 3\\) indicates potential outlier (If normally distributed, expect 99.7% &lt; 3)\n\n\n\nplot(\n    x = fitted(fit),\n    y = rstandard(fit),\n    ylab = 'Standardized Residuals',\n    xlab = 'Fitted values'\n    );\nabline(h = -3, lty = 2);\nabline(h = 3, lty = 2);"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#cooks-distance",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#cooks-distance",
    "title": "Regression Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\n\\(D_i = \\frac{\\sum_j(\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) * \\hat{\\sigma}^2}\\)\n\\(D_i\\) is proportional to the distance that the predicted values would move if the \\(i\\)th patient was excluded\nVarious cutoffs have been used in the literature, e.g. 1, \\(\\frac{4}{N}\\), or \\(\\frac{4}{N - p - 1}\\).. or visually identify patients with relatively large vals\n\n\n\n\ncar::influenceIndexPlot(fit, vars = 'Cook');"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-influential-observations",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#remedies-for-influential-observations",
    "title": "Regression Diagnostics",
    "section": "Remedies for influential observations",
    "text": "Remedies for influential observations\n\nSensitivity analysis: fit 2 models that include or exclude influential obs. Do the results substantially change?\nRobust regression: include all patients but downweight influential observations\n\nR: https://www.john-fox.ca/Companion/appendices/Appendix-Robust-Regression.pdf\nquantreg::rq() for median regression"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#interactions",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#interactions",
    "title": "Regression Diagnostics",
    "section": "Interactions",
    "text": "Interactions\n\nSomeone should do a short talk on interactions!\nBy default, linear regression assumes no interactions between predictors.\nYou can manually add interaction terms to the model to investigate. A*B in R formula gives A + B + A:B\n\n\n\n# allow all variables to interact with Sex\nfit.interaction &lt;- lm(fev ~ (age + height.inches + smoke) * sex, data = fev);\nsjPlot::tab_model(fit.interaction, ci.hyphen = ',&nbsp;', p.style = 'scientific', digits.p = 2);\n\n\n\n\n\n\n\n\n\n\n \nfev\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-3.36\n-4.07, -2.65\n1.93e-19\n\n\nage\n0.06\n0.03, 0.08\n5.05e-06\n\n\nheight inches\n0.09\n0.07, 0.10\n7.83e-30\n\n\nsmoke [Yes]\n-0.07\n-0.22, 0.08\n3.75e-01\n\n\nsex [Male]\n-1.32\n-2.23, -0.41\n4.48e-03\n\n\nage * sex [Male]\n0.03\n-0.01, 0.06\n1.39e-01\n\n\nheight inches * sex\n[Male]\n0.02\n0.00, 0.04\n4.07e-02\n\n\nsmoke [Yes] * sex [Male]\n0.02\n-0.22, 0.25\n8.93e-01\n\n\nObservations\n654\n\n\nR2 / R2 adjusted\n0.786 / 0.783\n\n\n\n\n\n\n\n\n\nIf you suspect many interactions, might be better to use a machine learning model that automatically handles interactions like decision-trees or Random Forest"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#summary",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#summary",
    "title": "Regression Diagnostics",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\n\nAssumption \nAssessment \n\nSolution \n\n\n\n\nLinearity\ncar::residualPlots, want horizontal band around 0 for each predictor\n\n- Transform \\(Y\\) or \\(X\\) - GAM to automate linear/non-linear - Polynomials\n\n\nNormality of residuals\n- Histogram/density plot - Normal QQ plot\n\n- Large N - Transform \\(Y\\) or \\(X\\) - Make sure linear assumption met - Bootstrap CIs: confint(car::Boot(fit))\n\n\nEqual variance\n- car::residualPlot and car::spreadLevelPlot, want horizontal band around 0 - car::ncvTest\n\n- Transform \\(Y\\) using exponent from spreadLevelPlot - Robust standard errors or bootstrap CI\n\n\nIndependent residuals\nPlot residuals vs time or other suspected clustering variables\n\n- Robust sandwich standard errors for cluster effect - Linear mixed models\n\n\nMulticollinearity\n- Check correlation between predictors - car::vif, want VIF &lt; 10\n\n- Given 2 highly corr. predictors, only keep 1 - caret::findCorrelation to remove predictors with corr &gt; cutoff - PCA, regularized regression\n\n\nInfluential obs\n- Plot standardized residuals vs fitted values; |r| &gt; 3 outlier - Cook’s distance, car::influenceIndexPlot\n\n- Sensitivity analysis fitting models with/without influential obs - Robust regression to downweight influential obs: quantreg::rq\n\n\nInteractions\n- Manually add interaction terms, significant?\n\n- Manually add interaction terms - Stratify model by potential interaction terms - ML models that automatically handle interactions"
  },
  {
    "objectID": "presentations/regression-diagnostics/regression_diagnostics.html#extensions-to-general-linear-models",
    "href": "presentations/regression-diagnostics/regression_diagnostics.html#extensions-to-general-linear-models",
    "title": "Regression Diagnostics",
    "section": "Extensions to General Linear Models",
    "text": "Extensions to General Linear Models\n\nGeneral linear model (GLM): for binary, multi-category, ordinal, or count outcomes\ncar::residualPlots to assess linearity of each predictor. Want to see horizontal band around 0 with no patterns. For non-linearity, use polynomials or GAM.\ncar::vif to assess multicollinearity\ncar::influenceIndexPlot to assess influential observations\ncar::Boot for robust bootstrap confidence intervals\nGLM also makes assumptions about the variance.. if false, can use bootstrap or robust standard errors:\n\n\nlmtest::coeftest(fit, vcov = sandwich::vcovHC(fit))"
  },
  {
    "objectID": "research.html#interests",
    "href": "research.html#interests",
    "title": "Jaron Arbet, Ph.D.",
    "section": "",
    "text": "Machine learning and predictive modeling\nHigh-dimensional “Big Data”\nVariable selection\nMulti-omics cancer data integration\nRobust nonparametric statistics"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Jaron Arbet, Ph.D.",
    "section": "",
    "text": "Automatic feature selection and engineering with MARS\nInterpretable machine learning"
  },
  {
    "objectID": "temp.html",
    "href": "temp.html",
    "title": "hello",
    "section": "",
    "text": "hello\n\nimg &lt;- image_read(\"temp.tiff\")\nimage_write(img, \"temp-image.pdf\", format = \"pdf\")\nknitr::include_graphics(\"temp-image.pdf\")"
  },
  {
    "objectID": "presentations.html#machine-learning",
    "href": "presentations.html#machine-learning",
    "title": "Jaron Arbet, Ph.D.",
    "section": "",
    "text": "Automatic feature selection and engineering with MARS\nInterpretable machine learning"
  },
  {
    "objectID": "presentations.html#multiomics-data-integration",
    "href": "presentations.html#multiomics-data-integration",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Multiomics data integration",
    "text": "Multiomics data integration\n\nMultiomics cancer data analysis"
  },
  {
    "objectID": "presentations.html#regression",
    "href": "presentations.html#regression",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Regression",
    "text": "Regression\n\nRobust regression for noisy biological data\nLinear regression diagnostics"
  },
  {
    "objectID": "presentations.html#causal-inference",
    "href": "presentations.html#causal-inference",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Causal inference",
    "text": "Causal inference\n\nCausal inference with observational data using propensity score matching\nTriplet matching: propensity score matching with 3 groups"
  },
  {
    "objectID": "presentations.html#bayesian-statistics",
    "href": "presentations.html#bayesian-statistics",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nIntroduction to Bayesian statistics"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "jaron.arbet@gmail.com"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nDoctor of Philosophy – Econometrics and Business Statistics  Monash University Aug 2021 — Present\n\nNew principles and methods for complex data preparation and integration, with applications to official statistics, web-scrapped data and satellite raster images\nMonash Data Futures Institute PhD Top-Up Scholarship (2021-2024)\nExpected submission date: Nov 2024"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nData Scientist\nFreelance, Multiple Clients Jan 2020 – June 2021\nProviding data collection, pre-processing, exploratory analysis and modelling services to clients in the early R&D stages of developing data driven products. Projects include:\n\nData description and preliminary product feasibility insights for a start-up real estate bond platform; including assessing suitability of various property transaction databases for use in initial product prototype.\nDevelopment of statistical anomaly detection regimes and key historical insights from internet quality time-series data for use in parametric insurance products, including documenting analysis tools in an R package."
  },
  {
    "objectID": "cv.html#presentations-and-workshops",
    "href": "cv.html#presentations-and-workshops",
    "title": "Curriculum Vitae",
    "section": "PRESENTATIONS AND WORKSHOPS",
    "text": "PRESENTATIONS AND WORKSHOPS\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)"
  },
  {
    "objectID": "cv.html#references",
    "href": "cv.html#references",
    "title": "Curriculum Vitae",
    "section": "References",
    "text": "References\nAvailable upon request"
  },
  {
    "objectID": "research.html#test",
    "href": "research.html#test",
    "title": "Jaron Arbet, Ph.D.",
    "section": "Test",
    "text": "Test\n\n\n\n\n1. Lippitt, Carlson, Arbet, Fingerlin, Maier, & Kechris. (2024). Limitations of clustering with PCA and correlated noise. Journal of Statistical Computation and Simulation. https://doi.org/10.1080/00949655.2024.2329976\n\n\n2. Ni, Rogowitz, Farahmand, Kaizer, Arbet, Cunningham, Thomas, & Saxon. (2024). Weight loss outcomes in a veterans affairs pharmacotherapy-based weight management clinic. Journal of the Endocrine Society. https://doi.org/10.1210/jendso/bvae042\n\n\n3. Tohi, Sahrmann, Arbet, Kato, Lee, Peacock, Ginsburg, Pavlovich, Carroll, Bangma, Sugimoto, & Boutros. (2024). De-escalation of monitoring in active surveillance for prostate cancer: Results from the GAP3 consortium. European Urology Oncology. https://doi.org/10.1016/j.euo.2024.07.006\n\n\n4. Weiner, Agrawal, Wang, Sonni, Li, Arbet, Zhang, Proudfoot, Hong, Davicioni, Kane, Valle, Kishan, Pra, Ghadjar, Sweeney, Nickols, Karnes, Shen, … Reiter. (2024). Molecular hallmarks of prostate-specific membrane antigen in treatment-na&lt;u+00EF&gt;ve prostate cancer. European Urology. https://doi.org/10.1016/j.eururo.2024.09.005\n\n\n5. Chan, Dodson, Arbet, Boutros, & Xiao. (2023). Single-cell analysis in lung adenocarcinoma implicates RNA editing in cancer innate immunity and patient prognosis. Cancer Research. https://doi.org/10.1158/0008-5472.CAN-22-1062\n\n\n6. Gamallat, Choudhry, Li, Rokne, Alhajj, Abdelsalam, Ghosh, Arbet, Boutros, & Bismar. (2023). Serrate RNA effector molecule (SRRT) is associated with prostate cancer progression and is a predictor of poor prognosis in lethal prostate cancer. Cancers. https://doi.org/10.3390/cancers15102867\n\n\n7. Greca, Grau, Arbet, Liao, Sosa, Haugen, & Kitahara. (2023). Anthropometric, dietary, and lifestyle factors and risk of advanced thyroid cancer: The NIH-AARP diet and health cohort study. Clinical Endocrinology. https://doi.org/10.1111/cen.14970\n\n\n8. Creasy, Ostendorf, Blankenship, Grau, Arbet, Bessesen, Melanson, & Catenacci. (2022). Effect of sleep on weight loss and adherence to diet and physical activity recommendations during an 18-month behavioral weight loss intervention. International Journal of Obesity (2005). https://doi.org/10.1038/s41366-022-01141-z\n\n\n9. Grau, Arbet, Ostendorf, Blankenship, Panter, Catenacci, Melanson, & Creasy. (2022). Creating an algorithm to identify indices of sleep quantity and quality from a wearable armband in adults. Sleep Science (Sao Paulo, Brazil). https://doi.org/10.5935/1984-0063.20220052\n\n\n10. Lin, Arbet, Mroz, Liao, Restrepo, Mayer, Li, Barkes, Schrock, Hamzeh, Fingerlin, Carlson, & Maier. (2022). Clinical phenotyping in sarcoidosis using cluster analysis. Respiratory Research. https://doi.org/10.1186/s12931-022-01993-z\n\n\n11. Okamoto, Devoe, Seto, Minarchick, Wilson, Rothfuss, Mohning, Arbet, Kroehl, Visser, August, Thomas, Charry, Fleischer, Feser, Frazer-Abel, Norris, Cherrington, Janssen, … Demoruelle. (2022). Association of sputum neutrophil extracellular trap subsets with IgA anti-citrullinated protein antibodies in subjects at risk for rheumatoid arthritis. Arthritis & Rheumatology (Hoboken, N.J.). https://doi.org/10.1002/art.41948\n\n\n12. Wood, Arbet, Amura, Nodine, Collins, Orlando, Mayer, Stein, & Anderson. (2022). Multicenter study evaluating nitrous oxide use for labor analgesia at high- and low-altitude institutions. Anesthesia and Analgesia. https://doi.org/10.1213/ANE.0000000000005712\n\n\n13. Arbet, Zhuang, Litkowski, Saba, & Kechris. (2021). Comparing statistical tests for differential network analysis of gene modules. Frontiers in Genetics. https://doi.org/10.3389/fgene.2021.630215\n\n\n14. Carpenter, Frank, Williamson, Arbet, Wagner, Kechris, & Kroehl. (2021). tidyMicro: A pipeline for microbiome data analysis and visualization using the tidyverse in r. BMC Bioinformatics. https://doi.org/10.1186/s12859-021-03967-2\n\n\n15. Nodine, Arbet, Jenkins, Rosenthal, Carrington, Purcell, Lee, & Hoon. (2021). Graduate nursing student stressors during the COVID-19 pandemic. Journal of Professional Nursing : Official Journal of the American Association of Colleges of Nursing. https://doi.org/10.1016/j.profnurs.2021.04.008\n\n\n16. Ostendorf, Blankenship, Grau, Arbet, Mitchell, Creasy, Caldwell, Melanson, Phelan, Bessesen, & Catenacci. (2021). Predictors of long-term weight loss trajectories during a behavioral weight loss intervention: An exploratory analysis. Obesity Science & Practice. https://doi.org/10.1002/osp4.530\n\n\n17. Ramakrishnan, Arbet, Mace, Suresh, Smith, S., Soler, & Smith. (2021). Predicting olfactory loss in chronic rhinosinusitis using machine learning. Chemical Senses. https://doi.org/10.1093/chemse/bjab042\n\n\n18. Reed, Arbet, & Staubli. (2021). Clinical nurse specialists in the united states registered with a national provider identifier. Clinical Nurse Specialist CNS. https://doi.org/10.1097/NUR.0000000000000592\n\n\n19. Rosenthal, Lee, Jenkins, Arbet, Carrington, Hoon, Purcell, & Nodine. (2021). A survey of mental health in graduate nursing students during the COVID-19 pandemic. Nurse Educator. https://doi.org/10.1097/NNE.0000000000001013\n\n\n20. Schmanski, Roberts, Coors, Wicks, Arbet, Weber, Crooks, Barnes, & Taylor. (2021). Research participant understanding and engagement in an institutional, self-consent biobank model. Journal of Genetic Counseling. https://doi.org/10.1002/jgc4.1316\n\n\n21. Arbet, Brokamp, Meinzen-Derr, Trinkley, & Spratt. (2020). Lessons and tips for designing a machine learning study using EHR data. Journal of Clinical and Translational Science. https://doi.org/10.1017/cts.2020.513\n\n\n22. Arbet, McGue, & Basu. (2020). A robust and unified framework for estimating heritability in twin studies using generalized estimating equations. Statistics in Medicine. https://doi.org/10.1002/sim.8564\n\n\n23. Coleman-Minahan, Sheeder, Arbet, & McLemore. (2020). Interest in medication and aspiration abortion training among colorado nurse practitioners, nurse midwives, and physician assistants. Women’s Health Issues : Official Publication of the Jacobs Institute of Women’s Health. https://doi.org/10.1016/j.whi.2020.02.001\n\n\n24. Gance-Cleveland, Linton, Arbet, Stiller, & Sylvain. (2020). Predictors of overweight and obesity in childhood cancer survivors. Journal of Pediatric Oncology Nursing : Official Journal of the Association of Pediatric Oncology Nurses. https://doi.org/10.1177/1043454219897102\n\n\n25. Thomas, Zaman, Cornier, Catenacci, Tussey, Grau, Arbet, Broussard, & Rynders. (2020). Later meal and sleep timing predicts higher percent body fat. Nutrients. https://doi.org/10.3390/nu13010073\n\n\n26. James-Allan, Arbet, Teal, Powell, & Jansson. (2019). Insulin stimulates GLUT4 trafficking to the syncytiotrophoblast basal plasma membrane in the human placenta. The Journal of Clinical Endocrinology and Metabolism. https://doi.org/10.1210/jc.2018-02778\n\n\n27. Arbet, McGue, Chatterjee, & Basu. (2017). Resampling-based tests for lasso in genome-wide association studies. BMC Genetics. https://doi.org/10.1186/s12863-017-0533-3\n\n\n28. Grinde, Arbet, Green, O’Connell, Valcarcel, Westra, & Tintle. (2017). Illustrating, quantifying, and correcting for bias in post-hoc analysis of gene-based rare variant tests of association. Frontiers in Genetics. https://doi.org/10.3389/fgene.2017.00117\n\n\n29. Greco, Hainline, Arbet, Grinde, Benitez, & Tintle. (2016). A general approach for combining diverse rare variant association tests provides improved robustness across a wider range of genetic architectures. European Journal of Human Genetics : EJHG. https://doi.org/10.1038/ejhg.2015.194"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Robust regression for noisy biological data\n\n\n\n\n\n\nNonparametric\n\n\nRegression\n\n\n\nBiological variables are often non-normally distributed, contain outliers, may have lower limits of detection, excessive zeros or multiple modes. The proportional odds (PO) regression model can easily handle these issues and is a robust alternative to linear regression and other parametric models. Indeed, the PO model can be thought of as an extension of the robust Wilcoxon rank-sum test that can adjust for covariates. \n\n\n\n\n\nSep 6, 2024\n\n\n\n\n\n\n\nLinear regression diagnostics\n\n\n\n\n\n\nRegression\n\n\n\nThis presentation introduces linear regression, covering its core assumptions, diagnostic techniques for assessing them, and strategies for addressing violations. \n\n\n\n\n\nJul 25, 2024\n\n\n\n\n\n\n\nAutomatic feature selection and engineering with MARS\n\n\n\n\n\n\nVariable selection\n\n\nMachine Learning\n\n\nNonparametric\n\n\n\nMultivariate adaptive regression splines (MARS) is a nonparametric machine learning method that automatically identifies which features are important for predicting the target variable (feature selection) as well as what forms the features should take (feature engineering), i.e. whether a feature should have a linear, non-linear, additive, or interaction effect. It is fairly fast method for working with big data and produces an interpretable model (not a black box). \n\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\n\nCausal inference with observational data using propensity score matching\n\n\n\n\n\n\nCausal inference\n\n\n\nThis presentation introduces propensity score matching as a method for attempting causal inference with observational data. \n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\nIntroduction to Bayesian statistics\n\n\n\n\n\n\nBayesian statistics\n\n\n\nAn introduction to the basics of Bayesian statistics, including Bayes’ Theorem, the 3 key aspects of Bayesian analysis (prior, model and posterior), and a comparison with traditional Frequentist statistics. \n\n\n\n\n\nAug 25, 2022\n\n\n\n\n\n\n\nTriplet matching: propensity score matching with 3 groups\n\n\n\n\n\n\nCausal inference\n\n\n\nPropensity score matching is a popular method for attempting causal inference with observational data. However, the vast majority of implementations focus on matching only two groups for a binary exposure variable. This presentation introduces triplet matching, a generalization of propensity score matching to three groups. \n\n\n\n\n\nApr 22, 2022\n\n\n\n\n\n\n\nMulti-omics data integration\n\n\n\n\n\n\nMulti-omics\n\n\n\nMulti-omics data integration is the process of combining data from different omics platforms (e.g. genomics, transcriptomics, epigenomics) to gain a more comprehensive understanding of biological systems. This presentation explains how to download multi-omics data from the TCGA database. Then dimension reduction methods are considered (PCA, PLS and CCA), and are shown to identify groups of correlated features. Lastly, two machine learning methods are compared for using multi-omic features to predict survival in kidney cancer patients. \n\n\n\n\n\nNov 21, 2021\n\n\n\n\n\n\n\nInterpretable machine learning\n\n\n\n\n\n\nMachine Learning\n\n\n\nMany machine learning models are considered “black boxes”: data is input and predictions are output, but we have little insight into how the model makes its decisions. This presentation covers methods for interpreting models, including visualizing the relationships between predictors and outcome, variable importance scores, interaction scores to identify predictors that interact, and methods to explain how a model makes a prediction for a given subject (LIME and Shapley values). \n\n\n\n\n\nJan 21, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/index.html#machine-learning",
    "href": "presentations/index.html#machine-learning",
    "title": "Presentations",
    "section": "",
    "text": "Automatic feature selection and engineering with MARS\nInterpretable machine learning"
  },
  {
    "objectID": "presentations/index.html#multiomics-data-integration",
    "href": "presentations/index.html#multiomics-data-integration",
    "title": "Presentations",
    "section": "Multiomics data integration",
    "text": "Multiomics data integration\n\nMultiomics cancer data analysis"
  },
  {
    "objectID": "presentations/index.html#regression",
    "href": "presentations/index.html#regression",
    "title": "Presentations",
    "section": "Regression",
    "text": "Regression\n\nRobust regression for noisy biological data\nLinear regression diagnostics"
  },
  {
    "objectID": "presentations/index.html#causal-inference",
    "href": "presentations/index.html#causal-inference",
    "title": "Presentations",
    "section": "Causal inference",
    "text": "Causal inference\n\nCausal inference with observational data using propensity score matching\nTriplet matching: propensity score matching with 3 groups"
  },
  {
    "objectID": "presentations/index.html#bayesian-statistics",
    "href": "presentations/index.html#bayesian-statistics",
    "title": "Presentations",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\nIntroduction to Bayesian statistics"
  },
  {
    "objectID": "blog/posts/intro_survival.html",
    "href": "blog/posts/intro_survival.html",
    "title": "Introduction to survival (time-to-event) analysis",
    "section": "",
    "text": "Intro\n\nQuick intro\nExample R code for common survival analysis methods. Covers 3 of the most common methods used in survival analysis: log-rank test and Kaplan Meir plots for comparing survival curves between groups, and Cox proportional hazards regression model.\nlogrank test for comparing survival curves between groups\n\n\n\nHow to interpret hazard ratios\n\nhttps://journals.asm.org/doi/full/10.1128/aac.48.8.2787-2792.2004\nThe “hazard ratio” is the most common effect size used in survival analysis. Briefly, it compares the risk of having the event occur in the next instant between groups\n\n\n\nRegression for time-to-event outcomes\n\nhttps://www.nature.com/articles/s41592-022-01689-8\nIntro to the Cox proportional hazard model (most common regression model for survival data)\nAlso introduces the accelerated failure time model (AFTR). Although less commonly used , this model has a nice interpretation that uses “time ratios”: it compares the ratio of the average time of event in group 1 to the average time of event in group 2.\n\n\n\nMachine learning\nHere are 2 popular machine learning models used for time-to-event outcomes:\n\nRegularized Cox model (i.e. a modified version of the Cox model that performs automatic feature selection)\nRandom survival forest. Perhaps you’ve heard of “random forests” before, it is one of the most popular machine learning methods that has been shown to work well across a wide variety of applications.\n\nThe C-index can be used to assess the predictive accuracy of a survival model. It is similar to the AUROC in that it ranges from 0 to 1, with 0.5 indicating no predictive ability and 1 indicating perfect predictive ability. Most survival model R functions will calculate the C-index for you, but if not, there are a few R packages for calculating it and comparing C between different models: survcomp, compareC, survC1"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Possibly Significant",
    "section": "",
    "text": "Welcome to my blog Possibly Significant. This is place for me to test out new statistical ideas, share short R stats tutorials, and organize notes. This is not meant to be highly polished material, but is more a place for me to quickly organize my thoughts on various stats topics, share new ideas and short R scripts for common statistical analyses. Perhaps you will find something significant here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputing missing data\n\n\n\n\n\nIntroduction to methods for imputing missing values\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to survival (time-to-event) analysis\n\n\n\n\n\nThis is a collection of introductory resources on survival time-to-event analysis that I often share with students and colleagues who are new to the topic.\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/imputation.html",
    "href": "blog/posts/imputation.html",
    "title": "Imputing missing data",
    "section": "",
    "text": "Missing data analysis: Making it work in the real world\n\nmultiple imputation is the gold standard method for handling missing data but is computationally intensive.\nEM algorithm works well, but lack of general software.\nIf doing likelihood-based regression modeling, then as long as you adjust for variables that influence missingness, results will not be biased due to missing data (although patients with missing values are dropped from the model, thus decreasing power).\n\nImputation likely still better due to improved power (i.e. all patients can be included)."
  },
  {
    "objectID": "blog/posts/imputation.html#continuous",
    "href": "blog/posts/imputation.html#continuous",
    "title": "Imputing missing data",
    "section": "Continuous",
    "text": "Continuous\n\nset.seed(123);\n\ndata('geno', package = 'missMDA'); \n\n# impute.knn (features in rows, samples in cols)\n# this is my preferred method for high dimensional continuous data since it is relatively fast\nknn.imp &lt;- impute::impute.knn(t(geno))$data;\nknn.imp &lt;- data.frame(t(knn.imp), check.names = FALSE);\n\n# PCA (samples in rows, features in cols)\nncomp &lt;- missMDA::estim_ncpPCA(geno, ncp.min = 0, ncp.max = 6);\npca.imp &lt;- missMDA::imputePCA(geno, ncp = ncomp$ncp, scale = TRUE)$completeObs;\n\n# missForest\nmf.imp &lt;- missForest::missForest(geno)$ximp;"
  },
  {
    "objectID": "blog/posts/imputation.html#categorical",
    "href": "blog/posts/imputation.html#categorical",
    "title": "Imputing missing data",
    "section": "Categorical",
    "text": "Categorical\n\ndata('vnf', package = 'missMDA');\n\n# missForest\nmf.imp &lt;- missForest::missForest(vnf)$ximp;\n\n# MCA\n#ncomp &lt;- missMDA::estim_ncpMCA(vnf); # slow method to estimate number of components\nmca.imp &lt;- missMDA::imputeMCA(vnf, ncp = 3)$complete.obs;"
  },
  {
    "objectID": "blog/posts/imputation.html#mixed-continuouscategorical",
    "href": "blog/posts/imputation.html#mixed-continuouscategorical",
    "title": "Imputing missing data",
    "section": "Mixed continuous/categorical",
    "text": "Mixed continuous/categorical\n\ndata('snorena', package = 'missMDA'); \n\n# missForest\nmf.imp &lt;- missForest::missForest(snorena)$ximp;\n\n# FAMD\n#missMDA::estim_ncpFAMD(snorena); # slow method to estimate number of components\nfamd.imp &lt;- missMDA::imputeFAMD(snorena, ncp = 3)$completeObs"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Possibly Significant",
    "section": "",
    "text": "Welcome to my blog Possibly Significant. This is place for me to test out new statistical ideas, share short R stats tutorials, and organize notes. This is not meant to be highly polished material, but is more a place for me to quickly organize my thoughts on various stats topics, share new ideas and short R scripts for common statistical analyses. Perhaps you will find something significant here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputing missing data\n\n\n\n\n\nIntroduction to methods for imputing missing values\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to survival (time-to-event) analysis\n\n\n\n\n\nThis is a collection of introductory resources on survival time-to-event analysis that I often share with students and colleagues who are new to the topic.\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro_survival.html",
    "href": "posts/intro_survival.html",
    "title": "Introduction to survival (time-to-event) analysis",
    "section": "",
    "text": "Intro\n\nQuick intro\nExample R code for common survival analysis methods. Covers 3 of the most common methods used in survival analysis: log-rank test and Kaplan Meir plots for comparing survival curves between groups, and Cox proportional hazards regression model.\nlogrank test for comparing survival curves between groups\n\n\n\nHow to interpret hazard ratios\n\nhttps://journals.asm.org/doi/full/10.1128/aac.48.8.2787-2792.2004\nThe “hazard ratio” is the most common effect size used in survival analysis. Briefly, it compares the risk of having the event occur in the next instant between groups\n\n\n\nRegression for time-to-event outcomes\n\nhttps://www.nature.com/articles/s41592-022-01689-8\nIntro to the Cox proportional hazard model (most common regression model for survival data)\nAlso introduces the accelerated failure time model (AFTR). Although less commonly used , this model has a nice interpretation that uses “time ratios”: it compares the ratio of the average time of event in group 1 to the average time of event in group 2.\n\n\n\nMachine learning\nHere are 2 popular machine learning models used for time-to-event outcomes:\n\nRegularized Cox model (i.e. a modified version of the Cox model that performs automatic feature selection)\nRandom survival forest. Perhaps you’ve heard of “random forests” before, it is one of the most popular machine learning methods that has been shown to work well across a wide variety of applications.\n\nThe C-index can be used to assess the predictive accuracy of a survival model. It is similar to the AUROC in that it ranges from 0 to 1, with 0.5 indicating no predictive ability and 1 indicating perfect predictive ability. Most survival model R functions will calculate the C-index for you, but if not, there are a few R packages for calculating it and comparing C between different models: survcomp, compareC, survC1"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Possibly Significant",
    "section": "",
    "text": "Welcome to my blog Possibly Significant. This is place for me to test out new statistical ideas, share short R stats tutorials, and organize notes. This is not meant to be highly polished material, but is more a place for me to quickly organize my thoughts on various stats topics, share new ideas and short R scripts for common statistical analyses. Perhaps you will find something significant here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputing missing data\n\n\n\n\n\nIntroduction to methods for imputing missing values\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to survival (time-to-event) analysis\n\n\n\n\n\nThis is a collection of introductory resources on survival time-to-event analysis that I often share with students and colleagues who are new to the topic.\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/imputation.html",
    "href": "posts/imputation.html",
    "title": "Imputing missing data",
    "section": "",
    "text": "Missing data analysis: Making it work in the real world\n\nmultiple imputation is the gold standard method for handling missing data but is computationally intensive.\nEM algorithm works well, but lack of general software.\nIf doing likelihood-based regression modeling, then as long as you adjust for variables that influence missingness, results will not be biased due to missing data (although patients with missing values are dropped from the model, thus decreasing power).\n\nImputation likely still better due to improved power (i.e. all patients can be included)."
  },
  {
    "objectID": "posts/imputation.html#continuous",
    "href": "posts/imputation.html#continuous",
    "title": "Imputing missing data",
    "section": "Continuous",
    "text": "Continuous\n\nset.seed(123);\n\ndata('geno', package = 'missMDA'); \n\n# impute.knn (features in rows, samples in cols)\n# this is my preferred method for high dimensional continuous data since it is relatively fast\nknn.imp &lt;- impute::impute.knn(t(geno))$data;\nknn.imp &lt;- data.frame(t(knn.imp), check.names = FALSE);\n\n# PCA (samples in rows, features in cols)\nncomp &lt;- missMDA::estim_ncpPCA(geno, ncp.min = 0, ncp.max = 6);\npca.imp &lt;- missMDA::imputePCA(geno, ncp = ncomp$ncp, scale = TRUE)$completeObs;\n\n# missForest\nmf.imp &lt;- missForest::missForest(geno)$ximp;"
  },
  {
    "objectID": "posts/imputation.html#categorical",
    "href": "posts/imputation.html#categorical",
    "title": "Imputing missing data",
    "section": "Categorical",
    "text": "Categorical\n\ndata('vnf', package = 'missMDA');\n\n# missForest\nmf.imp &lt;- missForest::missForest(vnf)$ximp;\n\n# MCA\n#ncomp &lt;- missMDA::estim_ncpMCA(vnf); # slow method to estimate number of components\nmca.imp &lt;- missMDA::imputeMCA(vnf, ncp = 3)$complete.obs;"
  },
  {
    "objectID": "posts/imputation.html#mixed-continuouscategorical",
    "href": "posts/imputation.html#mixed-continuouscategorical",
    "title": "Imputing missing data",
    "section": "Mixed continuous/categorical",
    "text": "Mixed continuous/categorical\n\ndata('snorena', package = 'missMDA'); \n\n# missForest\nmf.imp &lt;- missForest::missForest(snorena)$ximp;\n\n# FAMD\n#missMDA::estim_ncpFAMD(snorena); # slow method to estimate number of components\nfamd.imp &lt;- missMDA::imputeFAMD(snorena, ncp = 3)$completeObs"
  },
  {
    "objectID": "posts/imputation/imputation.html",
    "href": "posts/imputation/imputation.html",
    "title": "Imputing missing data",
    "section": "",
    "text": "Missing data analysis: Making it work in the real world\n\nMultiple imputation is the gold standard method for handling missing data but is computationally intensive.\nEM algorithm works well, but lack of general software.\nIf doing likelihood-based regression modeling, then as long as you adjust for variables that influence missingness, results will not be biased due to missing data (although patients with missing values are dropped from the model, thus decreasing power).\n\nImputation likely still better due to improved power (i.e. all patients can be included)."
  },
  {
    "objectID": "posts/imputation/imputation.html#continuous",
    "href": "posts/imputation/imputation.html#continuous",
    "title": "Imputing missing data",
    "section": "Continuous",
    "text": "Continuous\n\nset.seed(123);\n\ndata('geno', package = 'missMDA'); \n\n# impute.knn (features in rows, samples in cols)\n# this is my preferred method for high dimensional continuous data since it is relatively fast\nknn.imp &lt;- impute::impute.knn(t(geno))$data;\nknn.imp &lt;- data.frame(t(knn.imp), check.names = FALSE);\n\n# PCA (samples in rows, features in cols)\nncomp &lt;- missMDA::estim_ncpPCA(geno, ncp.min = 0, ncp.max = 6);\npca.imp &lt;- missMDA::imputePCA(geno, ncp = ncomp$ncp, scale = TRUE)$completeObs;\n\n# missForest\nmf.imp &lt;- missForest::missForest(geno)$ximp;"
  },
  {
    "objectID": "posts/imputation/imputation.html#categorical",
    "href": "posts/imputation/imputation.html#categorical",
    "title": "Imputing missing data",
    "section": "Categorical",
    "text": "Categorical\n\ndata('vnf', package = 'missMDA');\n\n# missForest\nmf.imp &lt;- missForest::missForest(vnf)$ximp;\n\n# MCA\n#ncomp &lt;- missMDA::estim_ncpMCA(vnf); # slow method to estimate number of components\nmca.imp &lt;- missMDA::imputeMCA(vnf, ncp = 3)$complete.obs;"
  },
  {
    "objectID": "posts/imputation/imputation.html#mixed-continuouscategorical",
    "href": "posts/imputation/imputation.html#mixed-continuouscategorical",
    "title": "Imputing missing data",
    "section": "Mixed continuous/categorical",
    "text": "Mixed continuous/categorical\n\ndata('snorena', package = 'missMDA'); \n\n# missForest\nmf.imp &lt;- missForest::missForest(snorena)$ximp;\n\n# FAMD\n#missMDA::estim_ncpFAMD(snorena); # slow method to estimate number of components\nfamd.imp &lt;- missMDA::imputeFAMD(snorena, ncp = 3)$completeObs"
  },
  {
    "objectID": "posts/intro_survival/intro_survival.html",
    "href": "posts/intro_survival/intro_survival.html",
    "title": "Introduction to survival (time-to-event) analysis",
    "section": "",
    "text": "Intro\n\nQuick intro\nExample R code for common survival analysis methods. Covers 3 of the most common methods used in survival analysis: log-rank test and Kaplan Meir plots for comparing survival curves between groups, and Cox proportional hazards regression model.\nlogrank test for comparing survival curves between groups\n\n\n\nHow to interpret hazard ratios\n\nhttps://journals.asm.org/doi/full/10.1128/aac.48.8.2787-2792.2004\nThe “hazard ratio” is the most common effect size used in survival analysis. Briefly, it compares the risk of having the event occur in the next instant between groups\n\n\n\nRegression for time-to-event outcomes\n\nhttps://www.nature.com/articles/s41592-022-01689-8\nIntro to the Cox proportional hazard model (most common regression model for survival data)\nAlso introduces the accelerated failure time model (AFTR). Although less commonly used , this model has a nice interpretation that uses “time ratios”: it compares the ratio of the average time of event in group 1 to the average time of event in group 2.\n\n\n\nMachine learning\nHere are 2 popular machine learning models used for time-to-event outcomes:\n\nRegularized Cox model (i.e. a modified version of the Cox model that performs automatic feature selection)\nRandom survival forest. Perhaps you’ve heard of “random forests” before, it is one of the most popular machine learning methods that has been shown to work well across a wide variety of applications.\n\nThe C-index can be used to assess the predictive accuracy of a survival model. It is similar to the AUROC in that it ranges from 0 to 1, with 0.5 indicating no predictive ability and 1 indicating perfect predictive ability. Most survival model R functions will calculate the C-index for you, but if not, there are a few R packages for calculating it and comparing C between different models: survcomp, compareC, survC1"
  }
]